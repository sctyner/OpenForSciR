[
["index.html", "Open Forensic Science in R Prologue", " Open Forensic Science in R Editor: Sam Tyner, Ph.D. 2019-09-15 Prologue This book is for anyone looking to do forensic science analysis in a data-driven and open way. Whether you are a student, teacher, or scientist, this book is for you. We take the latest research, primarily from the Center for Statistics and Applications in Forensic Evidence (CSAFE) and the National Institute of Standards and Technology (NIST) and show you how to solve forensic science problems in R. The book makes some assumptions about you: You have some experience with R (R Core Team 2019). We don’t assume you are an expert by any means, but we do assume you are comfortable enough with R to install &amp; library packages, load data, identify different data structures, and to follow along with the code we present in each chapter. If you need help getting started with R, there are lots of free resources online, and CSAFE has some resources available here. You can install R for Windows, Mac, and Linux here for free. We also recommend you install RStudio, the wonderful free IDE (Integrated Development Environment) for R. If you want a deeper dive into R, take a walk through R for Data Science. If you really want to explore the depths, Advanced R is an excellent resource. You are interested in forensic science. Hopefully that’s why you’re here! You may only be interested in DNA or firearms, so we’ve split the book up into chapters by forensic science subfield. You also don’t have to be an expert in the field. We will explain the basics of the field in the introduction of each chapter. You can also download this book by clicking here or by cloning it on GitHub and follow along, running the code on your own computer. You care about open source software. This doesn’t really affect your ability to read this book, but it’s a nice quality to have. The purpose of this book is to make forensic science more accessible. Right now, most databases, algorithms, and programs that get used every day in forensic science are proprietary, meaning that only the owners know how these systems work, how they were made, and what the source code looks like. This closed approach has lead to miscarriages of justice. With this free online book that relies solely on open-source software for analysis, we hope to demonstrate the impact open source software can have on forensic science, both in research and in practice. And in this spirit of openness, we ask that you contribute if you find an error or want to add a chapter on a topic we did not cover. You can open an issue here or fork the book’s Github repository and submit your changes via a pull request. If you’d like to contribute, we ask that you follow our contributor code of conduct and these recommended practices from Jenny Bryan and Jim Hester of RStudio. This book free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. References "],
["about-the-authors.html", "About the Authors1 Sam Tyner, Ph.D. Heike Hofmann, Ph.D. Soyoung Park, Ph.D. Eric Hare, Ph.D. Xiao Hui Tai Karen Kafadar, Ph.D. Karen Pan Amanda Luby", " About the Authors1 Sam Tyner, Ph.D. Sam is a Postdoctoral Research Associate in the Center for Statistics and Applications in Forensic Evidence (CSAFE) at Iowa State University (ISU). She earned her Ph.D. in Statistics from ISU in 2017. In 2018, she received a fellowship from rOpenSci to create this free book for anyone interested in open source forensic science. For more about Sam, follow her on Twitter or GitHub, and visit her website. Heike Hofmann, Ph.D. Heike is a Professor of Statistics at Iowa State University. She is a member of the CSAFE faculty and a core member of the Bioinformatics and Computational Biology program at ISU. She is also a faculty member in the Human-Computer Interaction program and is on the ISU Data Science Curriculum Committee. Her primary research interests are data visualization, multivariate categorical data analysis, statistical computing, exploratory data analysis and interactive statistical graphics. Soyoung Park, Ph.D. Soyoung Park is a Postdoctoral Research Associate at CSAFE at Iowa State University. She received her Ph.D. in Statistics from ISU in 2018. Her research interests include applied statistics, data science, forensic science, image analysis, and machine learning. Her recent research focuses on pattern matching problems in two-dimensional shoe sole impressions and optimal matching criteria for chemical matching of forensic glass fragments. Eric Hare, Ph.D. Eric is the Chief Data Scientist at Omni Analytics. He earned his Ph.D. in Statistics and Computer Science from Iowa State University in 2017. For his dissertation, Eric assisted in the development of bulletr, the precursor to the x3ptools and bulletxtrctr packages introduced in Chapter @ref(#bullets), and the associated bullet matching models which are a focus of a section of this book. Xiao Hui Tai Xiao Hui is a Ph.D. student in the Department of Statistics and Data Science at Carnegie Mellon University. She is currently working on matching problems in forensic science. Previously she was a statistician with the Singapore government. Karen Kafadar, Ph.D. Karen Kafadar is Commonwealth Professor and Chair of Statistics at University of Virginia. She received her Ph.D. in Statistics from Princeton and has served on several National Academy of Science committees, including the ones that led to the reports Weighing Bullet Lead Evidence (2004), Strengthening Forensic Science in the United States: A Path Forward (2009), Evaluating Testing, Costs, and Benefits of Advanced Spectroscopic Portals (2011), and Identifying the Culprit: Assessing Eyewitness Identification (2014). She also has served on the governing boards for the American Statistical Association (ASA), Institute of Mathematical Statistics, International Statistical Institute, National Institute of Statistical Sciences, Forensic Science Standards Board, and is the 2019 ASA President. Karen Pan Karen Pan is a Ph.D. student in the Department of Statistics at University of Virginia working on statistical issues in forensic science relating to fingerprints and glass evidence. Amanda Luby Amanda is currently finishing her Ph.D. in Statistics &amp; Data Science at Carnegie Mellon University. Her dissertation focuses on statistical methods for understanding complex human decision-making, with applications in forensic evidence analysis. She will be an Assistant Professor of Statistics at Swarthmore College beginning August 2019. Information current as of May 20, 2019↩ "],
["intro.html", "Chapter 1 Introduction 1.1 R Packages 1.2 Terminology and Definitions 1.3 Forensic Science Problems", " Chapter 1 Introduction 1.1 R Packages Here are all of the R packages used in this book. Some are installed from CRAN, some are installed from Bioconductor, and some are installed from GitHub using the devtools package (Wickham, Hester, and Chang 2019). If you would like to download and compile this book on your own system, this chunk will install and load all of the necessary packages for you. If you have problems with installation of any of these packages, this is probably due to an individual computer or R version issue. If you experience any installation problems, the best thing to do is to search for the error message you get on the web to see if anyone else has already solved your problem. If the package you are having troubles with has a GitHub repository, you should search its Issues to see if the author of the package has encountered and/or solved your problem before. If not, community.rstudio.com and Stack Overflow tend to be the most trustworthy sources of troubleshooting information. If all else fails, you can post an issue on the Github repo for this book. However, this should be your last resort: the authors of this book are subject matter experts, not necessarily computer debugging experts for your specific system and installation. options(repos = &quot;http://cran.us.r-project.org&quot;) # check if a package is installed, if not install it from cran install_cran_missing &lt;- function(pkgname){ if(class(pkgname) != &quot;character&quot;){ stop(&quot;pkgname must be a character&quot;) } if (!require(pkgname, character.only = TRUE, quietly = TRUE)){ install.packages(pkgname, dep = T) if(!require(pkgname,character.only = TRUE)) stop(&quot;CRAN Package not found&quot;) } } # check if a package is installed, if not install it from Github install_dev_missing &lt;- function(pkgname, ghuser){ if(class(pkgname) != &quot;character&quot;){ stop(&quot;pkgname must be a character&quot;) } if (!require(pkgname, character.only = TRUE, quietly = TRUE)){ repo &lt;- paste(ghuser, pkgname, sep =&#39;/&#39;) devtools::install_github(repo) if(!require(pkgname,character.only = TRUE)) stop(&quot;Github repo for package not found&quot;) } } # need devtools to get some packages install_cran_missing(&quot;devtools&quot;) # packages used throughout the book install_cran_missing(&quot;tidyverse&quot;) # or install_dev_missing(&quot;tidyverse&quot;, &quot;tidyverse&quot;) install_dev_missing(&quot;gt&quot;, &quot;rstudio&quot;) # Chapter 2: DNA Validation # Watch out for dependency issues with RGtk2... install_cran_missing(&quot;strvalidator&quot;) # Chapter 3: Bullets install_cran_missing(&quot;x3ptools&quot;) # or install_dev_missing(&quot;x3ptools&quot;, &quot;heike&quot;) install_cran_missing(&quot;randomForest&quot;) install_dev_missing(&quot;bulletxtrctr&quot;, &quot;heike&quot;) # Chapter 4: Casings if (!requireNamespace(&quot;BiocManager&quot;, quietly = TRUE)){ install.packages(&quot;BiocManager&quot;) } BiocManager::install(&quot;EBImage&quot;, version = &quot;3.8&quot;) install_dev_missing(&quot;cartridges&quot;, &quot;xhtai&quot;) # Chapter 5: Fingerprints install_cran_missing(&quot;bmp&quot;) install_cran_missing(&quot;kableExtra&quot;) install_dev_missing(&quot;fingerprintr&quot;, &quot;kdp4be&quot;) # Chapter 6: Shoe install_dev_missing(&quot;shoeprintr&quot;, &quot;CSAFE-ISU&quot;) install_dev_missing(&quot;patchwork&quot;, &quot;thomasp85&quot;) # Chapter 7: Glass install_cran_missing(&quot;caret&quot;) install_cran_missing(&quot;GGally&quot;) install_cran_missing(&quot;stringr&quot;) install_dev_missing(&quot;csafethemes&quot;, &quot;csafe-isu&quot;) # Chapter 8: Decision Making install_dev_missing(&quot;blackboxstudyR&quot;, &quot;aluby&quot;) install_cran_missing(&quot;rstan&quot;) install_cran_missing(&quot;RColorBrewer&quot;) install_cran_missing(&quot;gridExtra&quot;) install_cran_missing(&quot;gghighlight&quot;) install_cran_missing(&quot;ggpubr&quot;) 1.2 Terminology and Definitions In the appendix, you will find a dedicated Glossary section. The terms are listed in the order that they appear in the book. Throughout the book, the first time you encounter a glossary phrase, the phrase will be a link to the glossary entry, similar to the way Wikipedia handles their links. If you feel a term is lacking a glossary entry, please file an issue on GitHub. 1.3 Forensic Science Problems The purpose of this section is to introduce the general type of forensic science problem that will be covered in the book. The common understanding of forensic science is that law enforcement uses it to help solve crimes, but the primary professionals in forensic science are scientists, not members of law enforcement. According to the American Academy of Forensic Sciences (AAFS), any forensic scientist is first and foremost a scientist, who communicates their knowledge and their tests results to lawyers, juries, and judges. Where a legal matter is concerned, law enforcement is in charge of answering a very different question than the forensic scientist examining the evidence. Specifically in the criminal context, law enforcement wants to know who committed the crime, while the forensic scientist wants to understand the nature of the evidence. In theory, the forensic scientist’s conclusion can be used by law enforcement, but law enforcement information should generally not be used by the scientist, with some exceptions per R. Cook et al. (1998). Law enforcement and the courts focus on the offense level hypotheses, which describe the crime and its possible perpetrators (R. Cook et al. 1998). By contrast, the forensic scientist devotes their attention most often to the source level hypotheses, which describe the evidence and its possible sources (R. Cook et al. 1998). In between the offense level and the source level are the activity level hypotheses, which describe an action associated with the crime and the persons who did that action. For each level, two or more disjoint hypotheses are considered. For example, consider the following set of hypotheses adapted from R. Cook et al. (1998): Hypothesis Level Competing Hypotheses Offense Mr X committed the burglary \"\" Another person committed the burglary Activity Mr X smashed window W at the scene of the burglary \"\" Mr X was not present when the window W was smashed Source The glass fragments came from window W \"\" The glass fragments came from another unknown broken glass object In this example, the forensic scientist does not know the source of the glass fragments being analyzed, while the police may know that they were found on Mr X’s clothes upon his arrest. The forensic scientist will likely have a sample from the window W and the small fragments, and will only be asked to determine if the two samples are distinguishable. Note that the scientist does not need to know any details from the case, such as where the burglary occurred, who the suspect is, or what was stolen, to analyze the two glass samples. Notice the distinction: the forensic scientist is not a member of law enforcement, so their main concern is not “catching the bad guy.” Instead, they are concerned with coming to the best conclusion using science. The problems discussed in this book concern the source level hypotheses exclusively: we are not interested in taking the place of law enforcement or legal professionals. We are chiefly concerned with science, not the law. Each chapter in this book is outlined as follows: Introduction: This section familiarizes the reader with the type of forensic evidence covered in the chapter. Basic terms and concepts are covered, as well as what is typically done in a forensic science setting or lab to evaluate this type of evidence. The introduction section is not comprehensive by any stretch of the imagination: enough information is provided to understand the rest of the chapter, but that is all. For detailed coverage of a particular type of forensic evidence covered here, please consult each chapter’s references section. Data: This section covers the form of the forensic evidence and describes how that evidence is converted into computer-readable format for analysis in R. It will also provide the reader with an example data set, including the type and structure of data objects that are required for the analyses. R Package(s): This section introduces the R package(s) required for performing the analysis of the forensic evidence discussed. In some cases, the chapter author discusses (and in most cases is the author of) an R package explicitly for the particular forensic analysis. In other cases, the R package(s) used was applied the particular forensic science application, but is broadly applicable to other data analyses. Drawing Conclusions: This section describes how to make a decision about the evidence under consideration. In some cases, there are score based methods used to make a decision, and in others, a likelihood ratio is used. Comparisons to the current practice in forensic science are also made where applicable. Case study: This section follows a piece of forensic evidence from reading the requisite data into R to drawing a conclusion about the source of the evidence. The author guides the reader step-by-step through the R code required to perform a complete analysis. References "],
["dnaval.html", "Chapter 2 Validation of DNA Interpretation Systems 2.1 Introduction 2.2 Data 2.3 R Package 2.4 Drawing Conclusions 2.5 Case Study", " Chapter 2 Validation of DNA Interpretation Systems Sam Tyner Acknowledgements This work would not have been possible without the excellent documentation of the strvalidator package (Hansson, Gill, and Egeland 2014). Thank you to the package’s author, Oskar Hansson, Ph.D, who has authored many, many supporting documents, tutorials, etc. for his strvalidator package. Thank you, Oskar! 2.1 Introduction The earliest documented use of DNA profiling in the legal system was an immigration dispute in the United Kingdom (Butler 2005). A young man of Ghanaian descent with family in the UK was believed to have forged his Ghanaian passport and had an expired British passport2. DNA profiling techniques developed by Sir Alec Jeffreys were used to prove that the he was indeed his mother’s son, and thus he did have a right to immigrate to the UK. The technique was subsequently used for many other parentage cases, and soon after, DNA profiling was used for the first time to convict someone of murder in 1986 (Butler 2009). When DNA profiling began, an individual’s blood sample was taken to create their DNA profile. Now, DNA can be taken by a cheek swab, and minute traces of touch DNA can tie a perpetrator to the scene of the crime. This is thanks to the polymerase chain reaction (PCR), a method of copying a DNA sample over and over again to amplify the genetic signal for profile extraction. Once a DNA sample is amplified with PCR, different DNA markers can be analyzed to make an identification. The standard for forensic DNA typing is to use short tandem repeats (STRs) as the DNA marker. Other markers, single nucleotide polymorphisms (SNPs) and the mitochondrial genome (mtDNA), have different uses. SNPs can be used to identify ancestry or visible traits of a human, while mtDNA is used in cases where DNA is highly degraded (Liu and Harbison 2018). Because STR is the standard, we dedicate the rest of this chapter to its methodology. 2.1.1 Procedure for DNA Analysis using STRs In order to understand the STR methodology, we first need to understand what is being analysed. We present the comparison of genetic and printed information from Butler (2009) in Table 2.1. When forensic scientists analyze a DNA sample, they are looking for repeated “words” or DNA sequences in different “paragraphs,” or loci. The locus information is stored in the chromosome, which is the “page” the genetic information is on. Your chromosomes exist in the nucleus of every cell in your body, just like a page is within a chapter in a book in a library. STR markers are a set of loci or genes. At each locus, the number of times a tetranucleotide sequence (e.g. AAGC) repeats is counted (Butler 2005). This count indicates the allele, or gene variation, at that particular locus. Table 2.1: Recreation of Table 2.1 from Butler (2009) Printed Information Genetic Information Library Body Book Cell Chapter Nucleus Page Chromosome Paragraph Locus or gene Word Short DNA sequence Letter DNA nucleotide In forensic DNA profiling, a particular set loci are examined for comparison. The number of loci depends on the equipment and method used to analyzed the DNA sample, but can be as high as 27 for the particular method we discuss here (Butler, Hill, and Coble 2012). As of January 1, 2017, there are 20 core loci in CODIS, the Combined DNA Index System, which is the FBI’s national program for DNA databases and software used in criminal justice systems across the United States (FBI 2017). These sets of loci were chosen because of their high variability in the population. To find the alleles at each loci, the DNA sample is amplified using PCR, and then run through capillary electrophoresis (CE). The result of CE is the DNA profile, with the alleles on each locus indicated by different colored peaks from a chemical dyeing process. The amplification process introduces random change known as slippage, which creates stutter peaks in the observed DNA profile that are different than the true allele peaks (Butler 2009). In addition, different labs may use different machines and materials in forensic analysis resulting in different measurements for the same DNA sample. Thus, the validation of methods and materials is a crucial step. According to the Scientific Working Group on DNA Analysis Methods (SWGDAM), “validation is a process by which a procedure is evaluated to determine its efficacy and reliability for forensic casework and/or database analysis” (SWGDAM 2016). Validation helps minimize error in forensic DNA analysis and helps keep results consistent across laboratories and materials. The process of validation for forensic DNA methodology is expensive, time consuming, and unstandardized, and the R package strvalidator was created to help solve these problems in forensic DNA analysis (Hansson, Gill, and Egeland 2014). The strvalidator package makes the validation process faster by automating data analysis with respect to “heterozygote balance, stutter ratio, inter-locus balance, and the stochastic threshold” (Hansson, Gill, and Egeland 2014). In the remainder of this chapter, we introduce the type of data to import for use of this package, the primary functions of the package, and show an example of each of the four aforementioned validation steps in R. 2.2 Data The strvalidator package takes files exported from the GeneMapper® software, or a similar expert system that exports tab-delimited text files, as inputs. The data exported from these software programs typically come in a very wide format, and on import it needs to be transformed into a long format more appropriate for data analysis. In Figure 2.1, we visualize the process of data being transformed from wide to long format and back. In wide format, variable values are column names, while in the long format these column names become part of the data. Figure 2.1: Animation heuristic showing the transformation from long form to wide form data and back. Code for GIF from Omni Analytics Group The strvalidator package contains import methods to make sure that the data imported from other software is in the right form for validation analysis. The GeneMapper® software creates one column for each possible allele observed at a locus and their corresponding sizes, heights, and data points. Once the data have been trimmed and slimmed, they look something like this: head(myDNAdata) ## Sample.Name Marker Dye Allele Height ## 1 03-A2.1 AMEL B X 27940 ## 2 03-A2.1 AMEL B Y 107 ## 3 03-A2.1 D3S1358 B OL 80 ## 4 03-A2.1 D3S1358 B OL 453 ## 5 03-A2.1 D3S1358 B OL 85 ## 6 03-A2.1 D3S1358 B OL 408 where Sample.Name is the name of the sample being analyzed Marker is the locus in the DNA analysis kit Dye is the dye channel for that locus Allele is the allele (# of sequence repeats) at that locus Height is the observed peak height after amplification in RFUs (RFU = Relative Fluorescence Unit) 2.3 R Package The package strvalidator has a graphical user interface (GUI) to perform analyses so that no coding knowledge is necessary to run these analyses. The author of the package, Oskar Hansson, has written an extensive tutorial3 on the GUI. As this book is focused on open science, we do not use the GUI because it does not output the underlying code used for the point-and-click analyses. Instead, we use the code that powers the GUI directly. This code is called the “basic layer” of the package by Hansson, Gill, and Egeland (2014). The data are read into R via the import() function. This function combines the processes of trimming and slimming the data. Trimming selects the columns of interest for your analysis (e.g. Sample.Name, Allele, Height), while slimming converts the data from wide format to long format, as shown in Figure 2.1. After the data has be loaded, there are four main families of functions in the strvalidator package that are used for analysis. add*(): Add to the DNA data. For example, use addMarker() to add locus information or addSize() to add the fragment size in base pair (bp) for each allele. calculate*(): Compute properties of the DNA data. For example, use calculateHb() to compute heterozygous balance for the data or calculateLb() to compute the inter-locus balance (profile balance) of the data. remove*() : Remove artifacts from the data with removeArtefact() and remove spikes from the data with removeSpike(). table*() : Summarize results from one of the calculate*() analyses. For example, tableStutter() summarizes the results from calculateStutter(). For complete definitions and explanations of all functions available in strvalidator, please see the strvalidator manual. There are many other capabilities of strvalidator that do not cover in this chapter for the sake of brevity. 2.4 Drawing Conclusions There is no one tidy way to conclude a DNA validation analysis, which may be done for new machines, new kits, or any internal validation required. The strvalidator package’s primary purpose is to import large validation data sets and analyze the results of the validation experiment according to different metrics (Riman et al. 2016). A more complete description of the necessary validation studies is found in SWGDAM (2016), and full step-by-step tutorials can be found in Riman et al. (2016) and Hansson (2018). For validation analysis with respect to heterozygote balance, stutter ratio, inter-locus balance, and stochastic threshold, there are recommended guidelines to follow. 2.4.1 Stutter ratio The *Stutter() functions in strvalidator can analyze ratios of different types of stutter such as the backward stutter, the forward stutter, and the allowed overlap (none, stutter, or allele), as shown in Figure 2.4. Each of Hill et al. (2011), Westen et al. (2012), Brookes et al. (2012), and Tvedebrink et al. (2012) show greater stutter with more repeats, and these results are similar to those in Hansson, Gill, and Egeland (2014). In addition they found that some loci, such as TH01, experience less stutter on average than others. 2.4.2 Heterozygote balance For guidelines specific to the PowerPlex® ESX 17 and ESI 17 systems featured in Hansson, Gill, and Egeland (2014), refer to Hill et al. (2011). Generally speaking, per Gill, Sparkes, and Kimpton (1997), the heterozygote balance should be no less less than 60%. 2.4.3 Inter-locus balance Per Hansson, Gill, and Egeland (2014), there are two methods in strvalidator to compute inter-locus balance. As the proportion of the total peak height of a profile Relative to the highest peak total within a single locus in the profile, with the option to compute this value for each dye channel. Ideally, the loci would be perfectly balanced and the total peak height in each locus would be equal to \\(\\frac{1}{n}\\) where \\(n\\) is the number of loci in the kit (Hansson, Gill, and Egeland 2014). 2.4.4 Stochastic threshold The stochastic threshold (ST) or interpretation threshold is the “point above which there is a low probability that the second allele in a truly heterozygous sample has not dropped out” (Butler 2009). The ST is used to assess dropout risk in strvalidator. Another important threshold in DNA interpretation is the analytical threshold (AT), which is a peak height (for example, 50 RFUs) above which peaks “are considered an analytical signal and thus recorded by the data analysis software” (Butler 2009). Hansson, Gill, and Egeland (2014) refer to analytical threshold (AT) as the limit of detection threshold (LDT). Peaks above the AT are considered signal, and any peaks below the AT are considered noise. The ST is the RFU value above which it is reasonable to assume that, at a given locus, allelic dropout of a sister allele has not occurred. .4 Peaks that appear to be homozygous but have heights above the AT and below the ST may not be true homozygotes and may have experienced stochastic effects, such as allele dropout or elevated stutter. Usually, these stochastic events only happen for very small amounts of DNA that have been amplified. In strvalidator, dropout is scored according to the user-provided LDT value and the reference data provided. The risk of dropout is then modeled using a logistic regression of the calculated dropout score on the allele heights. Then for an acceptable level of dropout risk, say 1%, the stochastic threshold is computed according to the logistic regression model. Thus, the ST is the peak height at which the probability of dropout is less than or equal to 1%. 2.5 Case Study We do a simple case study using eight repeated samples from the same individual that are included in the strvalidator package. 2.5.1 Get the data We’ll use the package data set1, which is data from the genotyping of eight replicate measurements of a positive control sample, one replicate of a negative control sample, and the ladder used in analysis. The PowerPlex® ESX 17 System from the Promega Corporation5 was used on these samples for amplification of 17 loci recommended for analysis by the European Network of Forensic Science Institutes (ENFSI) and the European DNA Profiling Group (EDNAP), the European equivalent of SWGDAM. The known reference sample used is the ref1 data in the strvalidator package. First, we load the data, then slim it for analysis. Then, we use generateEPG() to visualize an electropherogram-like plot of the data. This function, like the other plotting functions in strvalidator, is built on the ggplot2 package (Wickham, Chang, et al. 2019). We also use the dplyr package throughout for data manipulation tasks (Wickham, François, et al. 2019). library(strvalidator) library(dplyr) library(ggplot2) data(set1) head(set1) ## Sample.Name Marker Dye Allele.1 Allele.2 Allele.3 Allele.4 Allele.5 ## 1 PC1 AMEL B X OL Y &lt;NA&gt; &lt;NA&gt; ## 2 PC1 D3S1358 B 16 17 18 &lt;NA&gt; &lt;NA&gt; ## 3 PC1 TH01 B 6 9.3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 PC1 D21S11 B 28 29 30.2 31.2 &lt;NA&gt; ## 5 PC1 D18S51 B 15 16 17 18 &lt;NA&gt; ## 6 PC1 D10S1248 G 12 13 14 15 &lt;NA&gt; ## Height.1 Height.2 Height.3 Height.4 Height.5 ## 1 2486 81 2850 &lt;NA&gt; &lt;NA&gt; ## 2 260 3251 2985 &lt;NA&gt; &lt;NA&gt; ## 3 3357 2687 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 183 2036 180 1942 &lt;NA&gt; ## 5 161 2051 203 1617 &lt;NA&gt; ## 6 168 2142 243 2230 &lt;NA&gt; # slim and trim the data set1.slim &lt;- slim(set1, fix = c(&quot;Sample.Name&quot;, &quot;Marker&quot;, &quot;Dye&quot;), stack = c(&quot;Allele&quot;, &quot;Height&quot;), keep.na = FALSE) dim(set1) ## [1] 170 13 dim(set1.slim) ## [1] 575 5 head(set1.slim) ## Sample.Name Marker Dye Allele Height ## 1 PC1 AMEL B X 2486 ## 2 PC1 AMEL B OL 81 ## 3 PC1 AMEL B Y 2850 ## 4 PC1 D3S1358 B 16 260 ## 5 PC1 D3S1358 B 17 3251 ## 6 PC1 D3S1358 B 18 2985 p &lt;- set1.slim %&gt;% filter(Sample.Name != &quot;Ladder&quot;) %&gt;% generateEPG(kit = &quot;ESX17&quot;) p + ggtitle(&quot;Mean peak heights for 8 samples from PC shown&quot;) Figure 2.2: Electropherogram-like ggplot2 plot of the mean of all 8 samples in set1 Next, get the reference sample data. data(ref1) head(ref1) ## Sample.Name Marker Allele.1 Allele.2 ## 1 PC AMEL X Y ## 2 PC D3S1358 17 18 ## 3 PC TH01 6 9.3 ## 4 PC D21S11 29 31.2 ## 5 PC D18S51 16 18 ## 6 PC D10S1248 13 15 ref1.slim &lt;- slim(ref1, fix = c(&quot;Sample.Name&quot;, &quot;Marker&quot;), stack = &quot;Allele&quot;, keep.na = FALSE) head(ref1.slim) ## Sample.Name Marker Allele ## 1 PC AMEL X ## 2 PC AMEL Y ## 3 PC D3S1358 17 ## 4 PC D3S1358 18 ## 5 PC TH01 6 ## 6 PC TH01 9.3 p &lt;- generateEPG(ref1.slim, kit = &quot;ESX17&quot;) + ggtitle(&quot;True profile for sample PC&quot;) p Figure 2.3: The reference profile electrogpherogram, ref1. 2.5.2 Check the stutter ratio Figure 2.4: Figure 2 from Hansson, Gill, and Egeland (2014). The analysis range, 2 back stutters and 1 forward stutter is shown at 3 levels of overlap. Stutter peaks are byproducts of the DNA amplification process, and their presence muddles data interpretation (Hansson, Gill, and Egeland 2014). Stutter is caused by strand slippage in PCR (Butler 2009). This slippage causes small peaks to appear next to true peaks, and a threshold is needed to determine if a peak is caused by slippage or if it could be a mixture sample with a minor contributor. We calculate the stutter for the eight replicates in set1 using one back stutter, no forward stutter and no overlap. We compare these values to the 95\\(^{th}\\) percentiles in Table 3 of Hansson, Gill, and Egeland (2014). See Figure 2.4 for an example of stutter. # make sure the right samples are being analyzed checkSubset(data = set1.slim, ref = ref1.slim) ## Reference name: PC ## Subsetted samples: PC1, PC2, PC3, PC4, PC5, PC6, PC7, PC8 # supply the false stutter and true stutter values for your data. these are # from the GUI. stutter_false_val &lt;- c(-1.9, -1.8, -1.7, -0.9, -0.8, -0.7, 0.9, 0.8, 0.7) stutter_replace_val &lt;- c(-1.3, -1.2, -1.1, -0.3, -0.2, -0.1, 0.3, 0.2, 0.1) # calculate the stutter values set1_stutter &lt;- calculateStutter(set1.slim, ref1.slim, back = 1, forward = 0, interference = 0, replace.val = stutter_false_val, by.val = stutter_replace_val) stutterplot &lt;- addColor(set1_stutter, kit = &quot;ESX17&quot;) %&gt;% sortMarker(kit = &quot;ESX17&quot;, add.missing.levels = FALSE) marks &lt;- levels(stutterplot$Marker)[-1] stutterplot$Marker &lt;- factor(as.character(stutterplot$Marker), levels = marks) compare_dat &lt;- data.frame(Marker = ref1$Marker[-1], perc95 = (c(11.9, 4.6, 10.9, 10.7, 12.1, 12, 11.1, 10.4, 16, 11.4, 9.1, 10.1, 8.3, 14.4, 10.1, 12.8))/100) compare_dat &lt;- filter(compare_dat, Marker %in% stutterplot$Marker) ggplot() + geom_point(data = stutterplot, position = position_jitter(width = 0.1), aes(x = Allele, y = Ratio, color = as.factor(Type)), alpha = 0.7) + geom_hline(data = compare_dat, aes(yintercept = perc95), linetype = &quot;dotted&quot;) + facet_wrap(~Marker, ncol = 4, scales = &quot;free_x&quot;, drop = FALSE) + labs(x = &quot;True Allele&quot;, y = &quot;Stutter Ratio&quot;, color = &quot;Type&quot;) Figure 2.5: Stutter ratios by allele for each of the eight samples in the set1 data, computed for one back stutter, zero forward stutter, and no overlap. Note that SR increases with allele length (e.g. D10S1248; D2S1338; D12S391). Horizontal dotted lines represent the 95th percentile of stutter ratio values from the study done in Hansson, Gill, and Egeland (2014). Figure 2.5 shows the ratio of stutter for each of the eight control samples in set1. The horizontal dotted lines show the 95\\(^{th}\\) percentile of the stutter ratio values computed in the same way from 220 samples in Hansson, Gill, and Egeland (2014). There are a few stutter values above the dotted line, but overall the values correspond to what we expect to happen in a sample with only one contributor. Unusual values are shown in Table 2.2. Table 2.2: Stutter peaks larger than the 95\\(^{th}\\) percentile of peak values for the study in Hansson, Gill, and Egeland (2014). Sample.Name Marker Allele HeightA Stutter HeightS Ratio Type 95th perc. PC1 D18S51 18 1617 17 203 0.126 -1 0.107 PC3 D18S51 18 1681 17 195 0.116 -1 0.107 PC4 D2S1338 25 3133 24 352 0.112 -1 0.111 PC5 D12S391 23 4378 22 640 0.146 -1 0.144 PC6 D2S1338 25 2337 24 261 0.112 -1 0.111 PC6 vWA 19 1571 18 196 0.125 -1 0.114 2.5.3 Check heterozygote balance (intra-locus balance) Computing the heterozygote peak balance (Hb) is most important for analyzing samples with two or more contributors. We calculate Hb values for the eight repeated samples in set1 below using Equation 3 from Hansson, Gill, and Egeland (2014) to compute the ratio. # checkSubset(data = set3, ref = ref3) set1_hb &lt;- calculateHb(data = set1.slim, ref = ref1.slim, hb = 3, kit = &quot;ESX17&quot;, sex.rm = TRUE, qs.rm = TRUE, ignore.case = TRUE) hbplot &lt;- addColor(set1_hb, kit = &quot;ESX17&quot;) %&gt;% sortMarker(kit = &quot;ESX17&quot;, add.missing.levels = FALSE) hbplot$Marker &lt;- factor(as.character(hbplot$Marker), levels = marks) ggplot(data = hbplot) + geom_point(aes(x = MPH, y = Hb, color = Dye), position = position_jitter(width = 0.1)) + geom_hline(yintercept = 0.6, linetype = &quot;dotted&quot;) + facet_wrap(~Marker, nrow = 4, scales = &quot;free_x&quot;, drop = FALSE) + scale_color_manual(values = c(&quot;blue&quot;, &quot;green&quot;, &quot;black&quot;, &quot;red&quot;)) + labs(x = &quot;Mean Peak Height (RFU)&quot;, y = &quot;Ratio&quot;, color = &quot;Dye&quot;) + guides(color = guide_legend(nrow = 1)) + theme(axis.text.x = element_text(size = rel(0.8)), legend.position = &quot;top&quot;) Figure 2.6: Hb ratio values for the eight samples in set1. Most ratios are above the 0.6 threshold. Figure 2.6 shows the Hb values for the eight samples in set1. The balance ratio is typically no less than 0.6 according to Gill, Sparkes, and Kimpton (1997), but there are a few exceptions to this rule in the set1 sample, shown in Table 2.3 Table 2.3: Observations in the set1 data which have Hb value less than 0.6. Sample.Name Marker Dye Delta Small Large MPH Hb PC1 D12S391 R 5.0 2323 4017 3170.0 0.578 PC3 SE33 R 1.0 4017 6761 5389.0 0.594 PC6 D10S1248 G 2.0 1760 3071 2415.5 0.573 PC7 D21S11 B 2.2 1487 2678 2082.5 0.555 2.5.4 Check inter-locus balance Inter-locus balance (Lb) is a measure of peak balances across loci (Hansson, Gill, and Egeland 2014). The total height of the peaks in all loci should be spread evenly across each individual locus in a sample. In the set1 data, 17 loci are measured, thus each individual locus balance should be about \\(\\frac{1}{17}^{th}\\) of the total height of all peaks in RFUs. set1_lb &lt;- calculateLb(data = set1.slim, ref = ref1.slim, kit = &quot;ESX17&quot;, option = &quot;prop&quot;, by.dye = FALSE, ol.rm = TRUE, sex.rm = FALSE, qs.rm = TRUE, ignore.case = TRUE, na = 0) set1_height &lt;- calculateHeight(data = set1.slim, ref = ref1.slim, kit = &quot;ESX17&quot;, sex.rm = FALSE, qs.rm = TRUE, na.replace = 0) set1_lb &lt;- set1_lb %&gt;% left_join(set1_height %&gt;% select(Sample.Name:Marker, Dye, TPH, H, Expected, Proportion) %&gt;% distinct(), by = c(&quot;Sample.Name&quot;, &quot;Marker&quot;, &quot;Dye&quot;, TPPH = &quot;TPH&quot;)) set1_lb &lt;- sortMarker(set1_lb, kit = &quot;ESX17&quot;, add.missing.levels = TRUE) ggplot(set1_lb) + geom_boxplot(aes(x = Marker, y = Lb, color = Dye), alpha = 0.7) + scale_color_manual(values = c(&quot;blue&quot;, &quot;green&quot;, &quot;black&quot;, &quot;red&quot;)) + geom_hline(yintercept = 1/17, linetype = &quot;dotted&quot;) + theme(legend.position = &quot;top&quot;, axis.text.x = element_text(size = rel(0.8), angle = 270, hjust = 0, vjust = 0.5)) + labs(y = &quot;Lb (proportional method)&quot;) Figure 2.7: Inter-locus balance for the eight PC samples. At each locus, the value should be about 1/17. The peak heights should ideally be similar in each locus. The inter-locus balance for this kit should ideally be about \\(\\frac{1}{17} \\approx 0.059\\). This value is shown by the horizontal dotted line in Figure 2.7. However, the markers in the red dye channel have consistently higher than ideal peaks and those in the yellow channel have consistently lower than ideal peaks. 2.5.5 Check stochastic threshold The stochastic threshold is the value of interest for determining allele drop-out. If a peak is above the stochastic threshold, it is unlikely that an allele in a heterozygous sample “has dropped out” (Butler 2009). Allele drop-out occurs when the allele peak height is less than the limit of detection threshold (LDT). As recommended in Butler (2009), we use an LDT of 50. The stochastic threshold is modeled with a logistic regression. set1_do &lt;- calculateDropout(data = set1.slim, ref = ref1.slim, threshold = 50, method = &quot;1&quot;, kit = &quot;ESX17&quot;) table(set1_do$Dropout) ## ## 0 ## 264 In set1, there is no dropout, as the samples included are control samples, and thus enough DNA is present during amplification so there are no stochastic effects. For a more exciting dropout analysis, we use another data set with more appropriate information. The data set4 was created specifically for drop-out analysis, and contains 32 samples from three different reference profiles. The method = \"1\" argument computes dropout with respect to the low molecular weight allele in the locus. data(set4) data(ref4) set4_do &lt;- calculateDropout(data = set4, ref = ref4, threshold = 50, method = &quot;1&quot;, kit = &quot;ESX17&quot;) table(set4_do$Dropout) ## ## 0 1 2 ## 822 33 68 In the set4 data, 33 alleles dropped out (Dropout = 1), and locus dropout (Dropout = 2) occurred in 9 samples (68 alleles). In one sample, all loci dropped out, while only one locus dropped out in three samples. The locus which most commonly dropped out was D22S1045 in seven samples, while loci D19S433 and D8S1179 only dropped out in two samples each. The probability of allele drop-out is computed via logistic regression of the dropout score with respect to the method 1, on the the height of the allele with low molecular weight. The model parameters are also computed using the calculateT() function. This function also returns the smallest threshold value at which probability of dropout is less than or equal to a set value, typically 0.01 or 0.05, as well as a conservative threshold, which is the value at which the risk of observing a drop-out probability greater than the specified threshold limit is less than the set value of 0.01 or 0.05. set4_do2 &lt;- set4_do %&gt;% filter(Dropout != 2) %&gt;% rename(Dep = Method1, Exp = Height) do_mod &lt;- glm(Dep ~ Exp, family = binomial(&quot;logit&quot;), data = set4_do2) set4_ths &lt;- calculateT(set4_do2, pred.int = 0.98) Next, we compute predicted dropout probabilities \\(P(D)\\) and corresponding 95% confidence intervals and plot the results. xmin &lt;- min(set4_do2$Exp, na.rm = T) xmax &lt;- max(set4_do2$Exp, na.rm = T) predRange &lt;- data.frame(Exp = seq(xmin, xmax)) ypred &lt;- predict(do_mod, predRange, type = &quot;link&quot;, se.fit = TRUE) # 95% prediction interval ylower &lt;- plogis(ypred$fit - qnorm(1 - 0.05/2) * ypred$se) # Lower confidence limit. yupper &lt;- plogis(ypred$fit + qnorm(1 - 0.05/2) * ypred$se) # Upper confidence limit. # Calculate conservative prediction curve. yconservative &lt;- plogis(ypred$fit + qnorm(1 - 0.05) * ypred$se) # Calculate y values for plot. yplot &lt;- plogis(ypred$fit) # combine them into a data frame for plotting predictionDf &lt;- data.frame(Exp = predRange$Exp, Prob = yplot, yupper = yupper, ylower = ylower) # plot th_dat &lt;- data.frame(x = 500, y = 0.5, label = paste0(&quot;At &quot;, round(set4_ths[1], 0), &quot; RFUs,\\nthe estimated probability\\nof dropout is 0.01.&quot;)) ggplot(data = predictionDf, aes(x = Exp, y = Prob)) + geom_line() + geom_ribbon(fill = &quot;red&quot;, alpha = 0.4, aes(ymin = ylower, ymax = yupper)) + geom_vline(xintercept = set4_ths[1], linetype = &quot;dotted&quot;) + geom_text(data = th_dat, inherit.aes = FALSE, aes(x = x, y = y, label = label), hjust = 0) + xlim(c(0, 1500)) + labs(x = &quot;Peak Height (RFUs)&quot;, y = &quot;Probability of allele drop-out&quot;) Figure 2.8: Probability of dropout in set4 for peaks from 100-1500 RFUs. 95% confidence interval for drop-out probability shown in red. We can also look at a heat map of dropout for each marker by sample. All the loci in sample BC10.11 dropped-out, while most other samples have no dropout whatsoever. set4_do %&gt;% tidyr::separate(Sample.Name, into = c(&quot;num&quot;, &quot;name&quot;, &quot;num2&quot;)) %&gt;% mutate(Sample.Name = paste(name, num, ifelse(is.na(num2), &quot;&quot;, num2), sep = &quot;.&quot;)) %&gt;% ggplot(aes(x = Sample.Name, y = Marker, fill = as.factor(Dropout))) + geom_tile(color = &quot;white&quot;) + scale_fill_brewer(name = &quot;Dropout&quot;, palette = &quot;Set2&quot;, labels = c(&quot;none&quot;, &quot;allele&quot;, &quot;locus&quot;)) + theme(axis.text.x = element_text(size = rel(0.8), angle = 270, hjust = 0, vjust = 0.5), legend.position = &quot;top&quot;) Figure 2.9: Dropout for all samples in set4 by marker. References "],
["bullets.html", "Chapter 3 Firearms: bullets 3.1 Introduction 3.2 Data 3.3 R Package(s) 3.4 Drawing Conclusions 3.5 Case Study", " Chapter 3 Firearms: bullets Eric Hare, Heike Hofmann Figure 3.1: Close-up of a bullet under a Confocal Light Microscope in the Roy J Carver High-resolution microscopy lab at Iowa State University. Photo by Heike Hofmann. Source: forensicstats.org 3.1 Introduction When a bullet is fired from a gun barrel, small imperfections in the barrel leave striation marks on the bullet. These marks are expressed most in the area of the bullet that has the closest contact to the barrel. These engravings are assumed to be unique to individual gun barrels, and as a result, traditional forensic science methods have employed trained forensic examiners to assess the likelihood of two bullets being fired from the same barrel (a “match”). Conventionally, this has been done using the metric Consecutively Matching Striae(CMS) (Biasotti 1959). However, no official standards have been established to scientifically delineate a number that effectively separates matches from non-matches. Therefore, significant work has been done, and continues to be done, in order to add scientific rigor to the bullet matching process. The 2009 National Academy of Sciences Report (National Research Council 2009a) may have been the “call-to-arms” that the field needed. This report criticized the lack of rigor in the field at the time, but also described the “path forward”. As the authors saw it, the path forward included adoption of standards. A standard format to represent the structure of bullets opened the door for much of what you’ll read about in this chapter, including opening up the formerly unknown process of bullet matching to a much wider audience, and providing the foundations for truly automated, statistical algorithms to perform the procedure. In this chapter, we outline the new standard data format used to store three-dimensional bullet scans. We proceed by outlying relevant R packages for the processing and analysis of these scans. Finally, we discuss ways in which to draw conclusions based on these results, and tie it all together in the form of a relevant case study. 3.2 Data Data on both breech face impression and land engraved areas are available from the NIST Ballistics Toolmark Research Database (NBTRD) in the x3p (XML 3-D Surface Profile) format. The x3p format was designed to implement a standard for exchanging 3D profile data. It was adopted by the Open Forensic Metrology Consortium, or OpenFMC, a group of firearm forensics researchers whose aim is to establish best practices for researchers using metrology in forensic science. Figure 3.2 shows an illustration of the internal structure of the x3p file format. x3p files contain an XML data file with metadata on the bullet scans, as well as binary data containing the surface topology measurements. The metadata includes information on the scanning equipment and operator, as well as information on the resolution of the scans. Figure 3.2: An illustration of the internal structure of the x3p file format. x3p files contain an XML data file with metadata on the bullet scans, as well as binary data containing the surface topology measurements. Source: openGPS The use of the x3p format has positively impacted procedures relating to forensic analysis of bullets. Because the format is an open standard, researchers on a wide range of computing platforms can access and analyze the data. Due to the x3p container holding a rich set of metadata, the limitations of traditional “black box”-type file formats are eliminated. The source, parameters, and raw data contained within each 3D scan is readily available for critical analysis and examination. 3.3 R Package(s) The first R package created to read and process x3p files was x3pr (OpenFMC 2014). This package includes reading routines to read in both the data as well as the metadata of a particular bullet land. The package also has some plotting functions and a writing routine to create x3p files. A new package, x3ptools (Hofmann et al. 2018), was created to handle some limitations in x3pr and expand upon the functionality. A companion package, bulletxtrctr (Hofmann, Vanderplas, and Krishnan 2018), expands upon x3ptools and provides functions to perform an automated bullet analysis routine based on the algorithms described in Hare, Hofmann, and Carriquiry (2017). The two packages x3ptools and bulletxtrctr will be the focus of the remainder of this chapter. 3.3.1 x3ptools Although x3ptools isn’t written specifically for the purposes of handling bullet scans, it is the package of choice to begin a bullet analysis. In fact, the package itself is generic and can handle a wide range of data types that use the x3p container format. To begin, the package can be installed from CRAN (stable release) or GitHub (development version): # from CRAN: install.packages(&#39;x3ptools&#39;) install development version from # GitHub: devtools::install_github(&quot;heike/x3ptools&quot;) We load the package and use some built-in x3p data to get a feel for the package functionality. We will work with the Center for Statistical Applications in Forensic Evidence (CSAFE) logo. In its original colored form, the logo looks like Figure 3.3. Figure 3.3: The CSAFE logo. Source: CSAFE. A 3D version of this logo is available in x3ptools, where portions of the logo are raised and recessed. This makes for a good test case in introducing x3ptools and the idea behind 3D scans of objects, as we transition towards bullet analysis. library(tidyverse) library(x3ptools) logo &lt;- read_x3p(system.file(&quot;csafe-logo.x3p&quot;, package = &quot;x3ptools&quot;)) names(logo) ## [1] &quot;header.info&quot; &quot;surface.matrix&quot; &quot;feature.info&quot; &quot;general.info&quot; ## [5] &quot;matrix.info&quot; We can see that there are five elements to the list object returned: header.info - Provides us information on the resolution of the scan surface.matrix - The actual surface data of the scan feature.info - Properties of the scan itself general.info - Information on how the data was captured matrix.info - Some information expanding upon header.info The two most relevant for our purposes are header.info and surface.matrix. To begin to understand this container format better, we can use the image_x3p function to produce a visualization of the surface, shown in Figure 3.4. image_x3p(logo) Figure 3.4: 3D surface scan of the CSAFE logo. (Rendered image has been down-sampled to speed up page load.) We can use the function x3p_to_df in order to convert this structure into a standard R data frame, which will allow us to do any number of data manipulation and plotting routines. In this case, Figure 3.5 shows a simple scatter plot created with ggplot2 of the height measurements across the surface of the bullet. logo_df &lt;- x3p_to_df(logo) ggplot(data = logo_df, aes(x = x, y = y, color = value)) + geom_point() + scale_color_gradient(low = &quot;white&quot;, high = &quot;black&quot;) + theme_bw() Figure 3.5: A simple scatterplot created with ggplot2 of the height measurements across the surface of the bullet. A key feature of the data is that the value column represents the height of the pixel corresponding to the particular \\((x,y)\\) location. In this logo, we can see that the fingerprint section of the logo is raised above the background quite clearly. As we transition to operating on images of bullets, this will be important to note. One other important feature of the package is the ability to sample. Depending on the size and resolution of a particular scan, the resulting object could be quite large. This CSAFE logo, despite being a relatively small physical size, still results in a 310,479 row data frame. Though manageable, this means that certain routines, such as producing the above scatter plot, can be quite slow. When high resolution is not needed, we may elect to sample the data to reduce the resulting size. This can be done with the sample_x3p function. The function takes a parameter m to indicate the sampling factor to use. For example, a value of m = 4 will sample every 4th height value from the 3D scan, as illustrated in Figure 3.6. sample_logo &lt;- sample_x3p(logo, m = 4) sample_logo_df &lt;- x3p_to_df(sample_logo) ggplot(data = sample_logo_df, aes(x = x, y = y, color = value)) + geom_point() + scale_color_gradient(low = &quot;white&quot;, high = &quot;black&quot;) + theme_bw() Figure 3.6: A sampled scan of an x3p file extracted using the sample_x3p function. You can see the clarity of the resulting plot has noticeably declined, but the overall structure has been maintained. Depending on the application, this could be a solution for making a slow analytical process a bit faster. 3.3.2 bulletxtrctr As mentioned, we will use the bulletxtrctr package to process 3D surface scans of bullets. This package depends on x3ptools for reading and writing x3p files but otherwise focuses on statistical routines for matching bullets. The package is not yet available on CRAN, but can be installed from GitHub: devtools::install_github(&quot;heike/bulletxtrctr&quot;) To demonstrate the functionality of bulletxtrctr, we use data from the NBTRD at NIST. We download the surface scan for a bullet from the Hamby Study (Hamby, Brundage, and Thorpe 2009), using the read_bullet function, transform the measurements from meters to microns (x3p_m_to_mum), and rotate the images so that the long axis is the horizontal. Note that the object hamby252demo is a list object exported from bulletxtrctr that contains URLs to the NIST NBTRD. library(randomForest) library(bulletxtrctr) # note: length(hamby252demo[[1]]) is 6 br1_b1 &lt;- read_bullet(urllist = hamby252demo[[1]]) %&gt;% # x3p_m_to_mum: converts from meters to microns mutate(x3p = x3p %&gt;% purrr::map(.f = x3p_m_to_mum)) %&gt;% # rotate_x3p(angle = -90: change orientation by 90 degrees clockwise # y_flip_x3p: flip image to conform to new ISO norm (see ??y_flip_x3p) mutate(x3p = x3p %&gt;% purrr::map(.f = function(x) x %&gt;% rotate_x3p(angle = -90) %&gt;% y_flip_x3p())) When working with lots of bullet data, it’s important to stay organized when naming objects in your R session. The name of the object we just created is br1_b1. This indicates that we are looking at the first bullet (b1) that was fired from Barrel 1 (br1). A bullet is composed of a certain number of land engraved areas (LEAs), and each LEA is a separate file with a separate URL. So, the object br1_b1 contains nrow(br1_b1) (6) observations, one for each land engraved area, which compose the whole bullet scan. The rifling of the barrel induces these land engraved areas, which are a series of alternating raised and recessed portions on the fired bullet. In addition, manufacturing defects engrave striation marks on the bullet as it travels through the gun barrel when fired (AFTE Criteria for Identification Committee 1992b). Let’s take a quick look at what we see one the first bullet land (Figure 3.7). image_x3p(br1_b1$x3p[[1]]) Figure 3.7: Land 1 of Bullet 1 from Barrel 1 of the Hamby Study (Set 44). Source: NRBTD. (Rendered image has been down-sampled to speed up page load.) Immediately, we can clearly see the vertical striation marks. To better visualize these marks, we can extract a cross-section from the bullet and plot it in two dimensions. To accomplish this, bulletxtrctr provides us with a function x3p_crosscut_optimize to choose the ideal location at which to do so. cc_b11 &lt;- x3p_crosscut_optimize(br1_b1$x3p[[1]]) cc_b11 ## [1] 100 This value provides us with the location (in microns) of a horizontal line that the algorithm determines to be a good place to extract a cross-section. The two primary criteria for determining this are: The location should be close to the base of the bullet (\\(y = 0\\)) because the striation marks are most pronounced there. Cross-sections taken near this location should be similar to this cross-section (stability). The x3p_crosscut_optimize function looks for the first cross-section meeting this criteria, searching upwards from the base of the bullet land. With this value, we can extract and plot the cross-section, shown in Figure 3.8. ccdata_b11 &lt;- x3p_crosscut(br1_b1$x3p[[1]], y = cc_b11) ggplot(data = ccdata_b11, aes(x = x, y = value)) + geom_line() + theme_bw() Figure 3.8: Cross-section of the bullet land at the ideal cross-section location. Most of the scans exhibit the pattern that we see here, where there are “wedges” on the left and right side. The wedge area is called the shoulder, and it is the area separating the land engraved area (the curved region in the middle) from the groove (the area not scanned because it doesn’t exhibit striations). In other words, to better hone in on the striation marks along the land, we should subset this region to include only the middle curved land engraved area portion. Fortunately, bulletxtrctr provides us with functionality to automatically do that. First, we use the cc_locate_grooves function to detect the location of the grooves. This returns a list object, with one element being the two locations along the axis, and the lother element being the plot, given in Figure 3.9. grooves_b11 &lt;- cc_locate_grooves(ccdata_b11, return_plot = TRUE, method = &quot;middle&quot;) grooves_b11$plot Figure 3.9: Location of the grooves in our bullet scan, as detected by the get_grooves function. With the grooves detected, we can now smooth out the surface using locally estimated scatter plot smoothing (LOESS) (Cleveland 1979). Once we do so, we obtain what we call a bullet signature, Figure 3.10, representing the clearest picture yet of the striation marks along the surface of the land. b111_processed &lt;- cc_get_signature(ccdata = ccdata_b11, grooves = grooves_b11, span1 = 0.75, span2 = 0.03) %&gt;% filter(!is.na(sig), !is.na(raw_sig)) ggplot(data = b111_processed, aes(x = x, y = sig)) + geom_line() + theme_bw() Figure 3.10: LOESS-smoothed version of our bullet profile, called the bullet signature. The land signature is the element of analysis for feature extraction out of the bulletxtrctr package. With multiple bullet signatures, matches can quickly and easily be made using the sig_align function, in conjunction with the extract_feature family of functions, which we will discuss later on in the chapter. 3.4 Drawing Conclusions We have seen the process of extracting the signature of a bullet and plotting it using R. But recall that the application of these procedures demands an answer to the question of whether this bullet was fired from the same gun barrel as another bullet. The question becomes, does this bullet signature “match” the signature of another bullet with high probability? This answer could be derived quite seamlessly in an ideal world given a reference database of all bullets in existence that have been fired from all gun barrels. With this database, we would compute the signatures for all of them and we could then make probabilistic judgments based on the similarities of signatures fired from the same barrel versus those from different barrels. Without this database, the best we can do is to begin a large data collection process resulting in a reference database, such as the approach in the NIST NBTRD. To come to a conclusion about the source of two fired bullets, we need to quantify the similarity of two land signatures that were part of bullets fired from the same barrel. This will be the focus of the Case Study section. One other approach to drawing conclusions is to use the generated signatures as a supplement to the manual examination by trained forensic examiners. This semi-automated procedure maintains the valuable expertise of the examiner and provides a scientific backing to some of the conclusions made. In the cases where conclusions may differ, this can lead to either refinement of the examination procedure, or refinement of the automated algorithms described. 3.5 Case Study We will now walk through the process of performing a bullet match. Much of the code for this section has been adapted from the excellent bulletxtrctr README. We take two bullets with 6 lands each for comparison. Thus, there are 36 land-to-land comparisons to be made, of which 6 are known matches, and 30 are known non-matches. We begin by reading the bullets: # bullet 1 urllist1 &lt;- c(&quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/cd204983-465b-4ec3-9da8-cba515a779ff&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/0e72228c-5e39-4a42-8c4e-3da41a11f32c&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/b9d6e187-2de7-44e8-9b88-c83c29a8129d&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/fda92f6a-71ba-4735-ade0-02942d14d1e9&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/8fa798b4-c5bb-40e2-acf4-d9296865e8d4&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/81e817e5-15d8-409f-b5bd-d67c525941fe&quot;) # bullet 2 urllist2 &lt;- c(&quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/288341e0-0fdf-4b0c-bd26-b31ac8c43f72&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/c97ada55-3a35-44fd-adf3-ac27dd202522&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/8a1805d9-9d01-4427-8873-aef4a0bd323a&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/a116e448-18e1-4500-859c-38a5f5cc38fd&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/0b7182d3-1275-456e-a9b4-ae378105e4af&quot;, &quot;https://tsapps.nist.gov/NRBTD/Studies/BulletMeasurement/DownloadMeasurement/86934fcd-7317-4c74-86ae-f167dbc2f434&quot;) b1 &lt;- read_bullet(urllist = urllist1) b2 &lt;- read_bullet(urllist = urllist2) For ease of analysis, we bind the bullets in a single data frame, and identify them using numeric values inside the data frame. We also indicate the six different lands. b1$bullet &lt;- 1 b2$bullet &lt;- 2 b1$land &lt;- 1:6 b2$land &lt;- 1:6 bullets &lt;- rbind(b1, b2) As before, we want to rotate the bullets such that the long axis is along the horizontal, as the functions within bulletxtrctr assume this format. bullets &lt;- bullets %&gt;% mutate(x3p = x3p %&gt;% purrr::map(.f = x3p_m_to_mum)) %&gt;% mutate(x3p = x3p %&gt;% purrr::map(.f = function(x) x %&gt;% rotate_x3p(angle = -90) %&gt;% y_flip_x3p())) We extract the ideal cross-sections from all 12 bullet lands, which are shown in Figure 3.11. In each land, we see the standard curved pattern, with well defined and a pronounced shoulders indicating the cutoff location for extracting the land. bullets &lt;- bullets %&gt;% mutate(crosscut = x3p %&gt;% purrr::map_dbl(.f = x3p_crosscut_optimize)) bullets &lt;- bullets %&gt;% mutate(ccdata = purrr::map2(.x = x3p, .y = crosscut, .f = x3p_crosscut)) crosscuts &lt;- bullets %&gt;% tidyr::unnest(ccdata) ggplot(data = crosscuts, aes(x = x, y = value)) + geom_line() + facet_grid(bullet ~ land, labeller = &quot;label_both&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1, size = rel(0.9))) Figure 3.11: Ideal cross-sections for all 12 bullet lands. Next, with each of these profiles, we need to detect grooves to extract the bullet signature between them. In Figure 3.12, we can see that the groove locations of the 12 bullet lands appear to be detected well, such that the middle portion between the two vertical blue lines represents a good sample of the land-engraved area. bullets &lt;- bullets %&gt;% mutate(grooves = ccdata %&gt;% purrr::map(.f = cc_locate_grooves, method = &quot;middle&quot;, adjust = 30, return_plot = TRUE)) do.call(gridExtra::grid.arrange, lapply(bullets$grooves, `[[`, 2)) Figure 3.12: Groove locations of each of the 12 bullet lands. With the groove locations detected, we proceed as before by using LOESS to smooth out the curvature of the surface and focus on the striation marks. Figure 3.13 shows us the raw signatures of the 12 lands. The striation marks are much more visible now. bullets &lt;- bullets %&gt;% mutate(sigs = purrr::map2(.x = ccdata, .y = grooves, .f = function(x, y) { cc_get_signature(ccdata = x, grooves = y, span1 = 0.75, span2 = 0.03) })) signatures &lt;- bullets %&gt;% select(source, sigs) %&gt;% tidyr::unnest() bullet_info &lt;- bullets %&gt;% select(source, bullet, land) signatures %&gt;% filter(!is.na(sig), !is.na(raw_sig)) %&gt;% left_join(bullet_info, by = &quot;source&quot;) %&gt;% ggplot(aes(x = x)) + geom_line(aes(y = raw_sig), colour = &quot;grey70&quot;) + geom_line(aes(y = sig), colour = &quot;grey30&quot;) + facet_grid(bullet ~ land, labeller = &quot;label_both&quot;) + ylab(&quot;value&quot;) + ylim(c(-5, 5)) + theme_bw() Figure 3.13: Signatures for the 12 bullet lands. Light gray lines show the raw data, while the dark gray lines are the smoothed signatures. Because we are working with 12 signatures, our goal will be to align all pairwise comparisons (36 comparisons total) between the six lands in each bullet. Figure 3.14 shows the alignment of Bullet 2 Land 3 with Bullet 1 Land 2, two of the known matches. Immediately it is clear that the pattern of the signatures appears very similar between the two lands. bullets$bulletland &lt;- paste0(bullets$bullet, &quot;-&quot;, bullets$land) lands &lt;- unique(bullets$bulletland) comparisons &lt;- data.frame(expand.grid(land1 = lands, land2 = lands), stringsAsFactors = FALSE) comparisons &lt;- comparisons %&gt;% mutate(aligned = purrr::map2(.x = land1, .y = land2, .f = function(xx, yy) { land1 &lt;- bullets$sigs[bullets$bulletland == xx][[1]] land2 &lt;- bullets$sigs[bullets$bulletland == yy][[1]] land1$bullet &lt;- &quot;first-land&quot; land2$bullet &lt;- &quot;second-land&quot; sig_align(land1$sig, land2$sig) })) subset(comparisons, land1 == &quot;2-3&quot; &amp; land2 == &quot;1-2&quot;)$aligned[[1]]$lands %&gt;% mutate(`b2-l3` = sig1, `b1-l2` = sig2) %&gt;% select(-sig1, -sig2) %&gt;% tidyr::gather(sigs, value, `b2-l3`, `b1-l2`) %&gt;% ggplot(aes(x = x, y = value, colour = sigs)) + geom_line() + theme_bw() + scale_color_brewer(palette = &quot;Dark2&quot;) Figure 3.14: Alignment of two bullet lands (2-3 &amp; 1-2) Though the visual evidence is strong, we want to quantify the similarity. To do this, we’re going to use a number of functions which extract features from the aligned signatures of the bullets. We’ll extract the cross-correlation (extract_feature_ccf), the matching striation count (bulletxtrctr:::extract_helper_feature_n_striae), the non-matching striation count, and many more (extract_feature_*). comparisons &lt;- comparisons %&gt;% mutate(ccf0 = aligned %&gt;% purrr::map_dbl(.f = function(x) extract_feature_ccf(x$lands)), lag0 = aligned %&gt;% purrr::map_dbl(.f = function(x) extract_feature_lag(x$lands)), D0 = aligned %&gt;% purrr::map_dbl(.f = function(x) extract_feature_D(x$lands)), length0 = aligned %&gt;% purrr::map_dbl(.f = function(x) extract_feature_length(x$lands)), overlap0 = aligned %&gt;% purrr::map_dbl(.f = function(x) extract_feature_overlap(x$lands)), striae = aligned %&gt;% purrr::map(.f = sig_cms_max, span = 75), cms_per_mm = purrr::map2(striae, aligned, .f = function(s, a) { extract_feature_cms_per_mm(s$lines, a$lands, resolution = 1.5625) }), matches0 = striae %&gt;% purrr::map_dbl(.f = function(s) { bulletxtrctr:::extract_helper_feature_n_striae(s$lines, type = &quot;peak&quot;, match = TRUE) }), mismatches0 = striae %&gt;% purrr::map_dbl(.f = function(s) { bulletxtrctr:::extract_helper_feature_n_striae(s$lines, type = &quot;peak&quot;, match = FALSE) }), bulletA = gsub(&quot;([1-2])-([1-6])&quot;, &quot;\\\\1&quot;, land1), bulletB = gsub(&quot;([1-2])-([1-6])&quot;, &quot;\\\\1&quot;, land2), landA = gsub(&quot;([1-2])-([1-6])&quot;, &quot;\\\\2&quot;, land1), landB = gsub(&quot;([1-2])-([1-6])&quot;, &quot;\\\\2&quot;, land2)) We are now ready to begin matching the bullets. We’ll start by looking at Figure 3.15, which aligns the two bullets by bullet land and colors each of the cells (comparisons) by the cross-correlation function (CCF) value. Encouragingly, we see a diagonal pattern in the matrix, which is to be expected given the assumption that the bullet scans were collected by rotating the bullet and are stored in rotational order. Note that we are also comparing each land to itself (top left and bottom right) in two of the four panels, which as expected exhibit the highest CCF for matches. comparisons &lt;- comparisons %&gt;% mutate(features = purrr::map2(.x = aligned, .y = striae, .f = extract_features_all, resolution = 1.5625), legacy_features = purrr::map(striae, extract_features_all_legacy, resolution = 1.5625)) %&gt;% tidyr::unnest(legacy_features) comparisons %&gt;% ggplot(aes(x = landA, y = landB, fill = ccf)) + geom_tile() + scale_fill_gradient2(low = &quot;grey80&quot;, high = &quot;darkorange&quot;, midpoint = 0.5) + facet_grid(bulletB ~ bulletA, labeller = &quot;label_both&quot;) + xlab(&quot;Land A&quot;) + ylab(&quot;Land B&quot;) + theme(aspect.ratio = 1) Figure 3.15: Land-to-Land Comparison of the two bullets colored by the CCF. We can improve upon these results by using a trained random forest, bulletxtrctr::rtrees, which was introduced in Hare, Hofmann, and Carriquiry (2017) in order to assess the probability of a match between bullet lands. Figure 3.16 displays the random forest score, or match probability, of each of the land-to-land comparisons. The results are stronger than using only the CCF in this case. comparisons$rfscore &lt;- predict(bulletxtrctr::rtrees, newdata = comparisons, type = &quot;prob&quot;)[, 2] comparisons %&gt;% ggplot(aes(x = landA, y = landB, fill = rfscore)) + geom_tile() + scale_fill_gradient2(low = &quot;grey80&quot;, high = &quot;darkorange&quot;, midpoint = 0.5) + facet_grid(bulletB ~ bulletA, labeller = &quot;label_both&quot;) + xlab(&quot;Land A&quot;) + ylab(&quot;Land B&quot;) + theme(aspect.ratio = 1) Figure 3.16: Random forest matching probabilities of all land-to-land comparisons. Finally, we can visualize the accuracy of our comparisons by highlighting the cells in which they were in fact matches (same-source). Figure 3.17 shows this, indicating that for the comparison between the two bullets, a couple of the lands didn’t exhibit a high match probability. With that said, given that the other four lands exhibited a strong probability, this is high evidence these bullets were in fact fired from the same barrel. Methods for bullet-to-bullet matching using the random forest results of land-to-land comparisons are still in development at CSAFE. Currently, sequence average matching (SAM) from Sensofar Metrology (2016) is used in similar problems to compare the CCF values in sequence (by rotation of the bullet), and methods in development have been using SAM as a baseline. Figure 3.17: All Land-to-Land Comparisons of the bullets, highlighting same-source lands. References "],
["casings.html", "Chapter 4 Firearms: casings 4.1 Introduction 4.2 Data 4.3 R Package(s) 4.4 Drawing Conclusions 4.5 Case Study", " Chapter 4 Firearms: casings Xiao Hui Tai 4.1 Introduction Marks are left on cartridge cases due to the firing process of a gun, in a similar way that marks are left on bullets. In the case of cartridge cases, there are at least two types of marks that are of interest. First, the firing pin hits the primer at the base of the cartridge, leaving a firing pin impression. The subsequent explosion (which launches the bullet) also causes the cartridge case to be pressed against the breech block of the gun, leaving impressed marks known as breechface marks. Both these types of marks are thought to individualize a gun, hence law enforcement officers frequently collect cartridge cases from crime scenes, in hopes of connecting these to retrieved guns, or connecting crimes where the same weapon was used. In current practice, retrieved cartridge cases are entered into a national database called the National Integrated Ballistics Information Network (NIBIN), through a computer-based platform which was developed and is maintained by Ultra Electronics Forensic Technology Inc. (FTI). This platform captures an image of the “new” cartridge case and runs a proprietary search algorithm, returning a list of top ranked potential matches from the database. Firearms examiners then examine this list and the associated images, to make a judgment about which potential matches warrant further investigation. The physical cartridge cases associated with these images are then located and examined under a comparison microscope. The firearms examiner decides if there are any matches, based on whether there is “sufficient agreement” between the marks (AFTE Criteria for Identification Committee 1992a), and may bring this evidence to court. There has been much public criticism in recent years about the current system. For example, PCAST (PCAST 2016) expressed concern that there had been insufficient studies establishing the reliability of conclusions made by examiners, and the associated error rates had not been adequately estimated. They suggested two directions for the path forward. The first is to “continue to improve firearms analysis as a subjective method,” and the second is to “convert firearms analysis from a subjective method to an objective method,” through the use of automated methods and image-analysis algorithms (PCAST 2016). There have been efforts by various groups, both commercial and academic, in line with this second recommendation. A full review is out of the scope of the current text, but we refer the interested reader to Roth et al. (2015), Geradts et al. (2001), Thumwarin (2008), Riva and Champod (2014), Vorburger et al. (2007), Song (2013), and others. One point to note is that as far as we know, none of these methods are open-source. We have developed methodology to process and compare cartridge cases in a fully automatic, open-source manner, and in this chapter, we describe R packages to accomplish these tasks. 4.2 Data NIST maintains a Ballistics Toolmark Research Database (https://tsapps.nist.gov/NRBTD), an open-access research database of bullet and cartridge case toolmark data. The database contains images from test fires originating from studies conducted by various groups in the firearm and toolmark community. These cartridge cases were originally collected for different purposes, for example the Laura Lightstone study investigated whether firearms examiners were able to differentiate cartridge cases from consecutively manufactured pistol slides (Lightstone 2010). The majority of available data are cartridge cases that were sent to NIST for imaging, but the website also allows users to upload their own data in a standardized format. There are a total of 2,305 images (as of 3/4/2019), and among these are data sets involving consecutively manufactured pistol slides, a large number of firings (termed persistence studies because they investigate the persistence of marks), as well as different makes and models of guns and ammunition. Gun manufacturers in the database include Glock, Hi-Point, Ruger, Sig Sauer, and Smith &amp; Wesson, and ammunition brands include CCI, Federal, PMC, Remington, Speer, Wolf and Winchester. Measurements are primarily made using a Leica FS M 2D reflectance microscope, and a Nanofocus uSurf disc scanning confocal microscope. The former captures photo images while the latter captures 3D topographies. Detailed metadata are available for each of these images, for example for photo images, the magnification was 2X with a lateral resolution of \\(2.53 \\mu m\\), producing \\(2592 \\times 1944\\) pixel, 256-grayscale PNG images. For 3D, various magnifications were used, for example an objective of 10X results in a lateral resolution of \\(3.125 \\mu m\\), and images that are around \\(1200 \\times 1200\\). The 3D data are in x3p format, and more information about this file format can be found in Chapter 3. Examples of images are in Section 4.5. 4.3 R Package(s) The goal of the analysis is to derive a measure of similarity between a pair of cartridge case images. There are a few steps involved in such an analysis. Broadly, we first need to process the images so that they are ready for analysis. This might involve selecting relevant marks or highlighting specific features. Next, given two images, they need to be aligned so that any similarity measure extracted is meaningful. The final step is to estimate the similarity score. We have developed R packages to analyze images in the standard format in NIST’s database. cartridges analyzes 2D photo images, while cartridges3D analyzes 3D topographies. A complete description of methodology used in cartridges is in Tai and Eddy (2018). cartridges3D modifies this for 3D topographies, with the major difference being in pre-processing. More details can be found in the package README. The primary functions of the package are allPreprocess and calculateCCFmaxSearch. The former performs all pre-processing steps, while the latter does both alignment and computation of a similarity score. The corresponding function for processing 3D data is allPreprocess3D. The end result is a similarity score for a pair of images being compared. 4.4 Drawing Conclusions Depending on the goal of the analysis, as well as the availability of data, there are a few ways in which conclusions may be drawn. The analysis produces a similarity score for a pair of images. This could be sufficient for the analysis, for example if we have two pairs of images being compared, the goal might be simply to estimate which of the two pairs are more similar to each other. This situation is straightforward, and we can make a conclusion such as “Comparison 1 contains images that are more similar than Comparison 2.” If a set of comparisons are being done, the conclusion might be of the form “these are the top 10 pairs with highest similarity scores out of the 100 comparisons being made.” This could be used to generate investigative leads, where we select the top 10 (say) pairs for further investigation. A different context in which this type of conclusion could be used is by examiners for blind verification. This means that an examiner first comes to their own conclusion, and then verifies this using an automatic method, making a conclusion such as “Based on my experience and training, these two cartridge cases come from the same gun. This pair that I identified also had a score of .7, the highest similarity score returned by [[insert algorithm]] among [[insert subset of pairs being considered]].” In other situations, we might be interested in designating a similarity cutoff above which some action is taken. The selection of such a cutoff depends on the goal. For example, similar to the above situation, we might be interested in selecting pairs above a cutoff for further manual investigation, instead of simply picking the top 10 pairs. Alternatively, a cutoff could be used to decide if pairs are matches or non-matches. This could be of interest in criminal cases, where a conclusion of match or non-match is required to decide if a person should be implicated in a crime. In the first case a lower cutoff might be set to ensure high recall, while in the second case a much higher cutoff might be necessary. Given appropriate data on the distribution of similarity scores for non-matching pairs in some population of interest, a third type of conclusion that we can draw is to estimate a probability of getting a higher similarity score by chance. For example, if we obtain a similarity of .7 for the pair of interest, we compare .7 to some distribution of similarity scores for non-matching pairs, that might have been obtained from prior studies. The probability of interest is the probability that a random draw from that distribution is larger than .7, say \\(p_0\\). The conclusion that we can then draw is that if the pair was a non-match, the probability of getting a score higher than .7 is \\(p_0\\). If the value of \\(p_0\\) is small, this provides evidence against the hypothesis that the pair of interest is a non-matching pair. Such a probability can be used as a measure of the probative value of the evidence. 4.5 Case Study The following case study uses two 2D photo images from the NBIDE study in NIST’s database, coming from the same Ruger gun, firing PMC ammunition. Referring to the study metadata, these are cartridge cases RR054 and RR072, corresponding to the files “NBIDE R BF 054.png” and “NBIDE R BF 072.png”. We first load the package: library(cartridges) We can read in and plot the images as follows. If using a user-downloaded image, one can simply replace the file path with the location of the downloaded image. exImage1 &lt;- readCartridgeImage(&quot;./img/casings_NBIDE054.png&quot;) plotImage(exImage1, type = &quot;original&quot;) exImage2 &lt;- readCartridgeImage(&quot;./img/casings_NBIDE072.png&quot;) plotImage(exImage2, type = &quot;original&quot;) Now, all the pre-processing can be done using allPreprocess. processedEx1 &lt;- allPreprocess(&quot;./img/casings_NBIDE054.png&quot;) processedEx2 &lt;- allPreprocess(&quot;./img/casings_NBIDE072.png&quot;) The processed images can be plotted using plotImage. plotImage(processedEx1, type = &quot;any&quot;) plotImage(processedEx2, type = &quot;any&quot;) Now, to compare these two images, we use calculateCCFmaxSearch(processedEx1, processedEx2) This produces a score of .40. As discussed in Section 4.4, the conclusions to be drawn depend on the goals of the analysis, as well as the availability of data. The first type of conclusion could be that this pair of images is more similar to each other than some other pair of images. The second type of conclusion could be that this score is high enough to warrant further manual investigation. Finally, if we have some prior information on some reference distribution of non-matching scores, we can compute the probability of obtaining a higher score by chance as follows. Given a reference population of interest, one can perform the appropriate pairwise comparisons and obtain non-match distributions empirically. Here we use a normal distribution for purposes of illustration. set.seed(0) computeProb(0.4, rnorm(50, 0.02, 0.3)) ## [1] 0.08 The conclusion then is that the probability of obtaining a score higher than .40, for a non-matching pair, is .08. The same type of analysis can be done with 3D topographies using the corresponding functions in cartridges3D. References "],
["fingerprints.html", "Chapter 5 Latent Fingerprints 5.1 Introduction 5.2 Data 5.3 R Package 5.4 Drawing Conclusions 5.5 Case Study", " Chapter 5 Latent Fingerprints Karen Kafadar, Karen Pan 5.1 Introduction Latent fingerprints collected at crime scenes have been widely used for individual identification purposes, primarily because fingerprints have long been assumed to be unique to an individual. Thus it is assumed that some subset of features, or minutiae, on a print can be identified and will suffice to determine whether a latent print and a digital print from a database collected under controlled conditions (e.g., in a police laboratory) came from the “same source.” However, unlike digital fingerprints in a database, latent prints are generally of poor quality and incomplete, missing ridge structures or patterns. The quality of a latent print is needed for the “Analysis” and “Comparison” phases of the “ACE-V” fingerprint identification process (Friction Ridge Analysis Study and Technology 2012). This quality is currently judged visually, based on clarity of the print and specific features of it for identification purposes. This process, though done by trained fingerprint examiners, is nevertheless subjective and qualitative, as opposed to objective and quantitative. Once the examiner identifies seemingly usable minutiae on a latent print, the print is entered into an Automated Fingerprint Identification System (AFIS) which uses the examiner’s minutiae to return likely “matches” from a fingerprint database. Thus, the accuracy of the identification depends first and foremost on the quality and usability of the minutiae in a latent. More independent, high-quality features should lead to more accurate database “matches”. To date, neither objective measures of quality in selected minutiae, nor dependence among features, nor the number needed for high-accuracy calls, has been considered. Quoting from B. T. Ulery et al. (2011), “No such absolute criteria exist for judging whether the evidence is sufficient to reach a conclusion as opposed to making an inconclusive or no-value decision. The best information we have to evaluate the appropriateness of reaching a conclusion is the collective judgments of the experts.” A digital fingerprint acquisition system does provide a numerical “quality” score of an exemplar print at the time it is taken (to ensure adequate clarity for later comparison), but the “quality” of the latent fingerprint is typically assessed qualitatively by the examiner. Some authors have proposed measures of overall latent print quality. Bond (2008) defines a five-point scale primarily in terms of ridge continuity. Tabassi, Wilson, and Watson (2004) define a five-point quality scale in terms of contrast and clarity of features. The five-point scale reflects how the quality of the latent print impacts the ability of a matching algorithm to find and score matching prints: high [low] quality is associated with good [poor] match performance. Yoon, Liu, and Jain (2012) define a latent fingerprint image quality (LFIQ) score from a user-defined set of features based on clarity of ridges and features. Tabassi, Wilson, and Watson (2004) cite other latent print quality measures that have been proposed and conclude that, for all of them, “evaluating their quality measure is a subjective matter” (Tabassi, Wilson, and Watson 2004, 6). Nonetheless, there remains “substantial variability in the attributes of latent prints, in the capabilities of latent print examiners, in the types of casework received by agencies, and the procedures used among agencies” (Ulery et al. 2012). Consequently, some procedure that offers an objective measure of minutiae quality is needed. Different features (minutiae) on a latent print supply different amounts of information to an examiner. Our goal is to develop a quality metric for each feature, based on a measure of information in the feature. Visually, a feature on a print (ridge ending, bifurcation, etc.) is more recognizable when it is easily differentiated from the background around it. In subsequent sections we develop a quality metric for each latent fingerprint feature that quantifies its distinctiveness from its background value, and hence, how reliable a feature might be for purposes of comparison (step 2 of the ACE-V process). 5.1.1 Contrast Gradient Quality Measurement An algorithm introduced in Peskin and Kafadar (n.d.) identifies and examines a small collection of pixels surrounding a given feature and assesses their distinctiveness from the background pixels. The underlying principle for this approach lies in recognizing that a forensic examiner can distinguish features in a fairly blurry latent print by recognizing: a gradient of intensity between the dark and light regions defining a minutia point, and an overall contrast of intensity values in the general neighborhood of a minutiae point. The first step in the algorithm locates the pixel within a small neighborhood around the minutia location that produces the highest intensity gradient value. Further calculation is done around that pixel. The largest gradient in a neighborhood of 5 pixels in each direction from a feature is found. For each pixel in this neighborhood, we compare the pixel intensity, \\(i(x, y)\\), to a neighboring pixel intensity, \\(i(x + n, y + m)\\), where \\(n, m \\in \\{-2, -1, 0, 1, 2\\}\\), and then divide by the corresponding distance between the pixels to get a measure of the gradient at pixel location \\((x, y)\\), \\(g(x, y; n, m)\\): \\[ g(x, y; n, m) = \\frac{i(x, y) - i(x + n, y + m)}{ \\sqrt{n^2 + m^2}}, \\qquad n, m = -2, ..., 2. \\] Define \\(G_5(x, y)\\) as the set of all 24 gradients in the \\(5 \\times 5\\) neighborhood of \\((x, y)\\). The value used for the quality measurement is the maximum value in the set \\(G_5(x, y)\\). We define \\((x_0, y_0)\\) as the point that produces the largest gradient. The next step in the algorithm locates the largest contrast between the point \\((x_0, y_0)\\) and its immediate \\(3 \\times 3\\) neighborhood, \\((x_0 + n, y_0 + m)\\), or \\(n_3(x_0, y_0)\\), where \\(n,m \\in \\{-1, 0, 1\\}\\). The contrast factor is the largest intensity difference between \\(i(x_0, y_0)\\) and any neighbor intensity in \\(n_3(x_0, y_0)\\), divided by the maximum intensity in the print, \\(I_M\\), usually 255: \\[ contrast = \\frac{max\\{abs(i(x_0, y_0) - i(x,y)\\}}{I_M}, \\qquad (x, y) \\in n_3(x_0, y_0). \\] The contrast measurement differs from the gradient measurement because it highlights the maximum change in intensity among all nine points surrounding the minutia point at \\((x_0, y_0)\\), while the gradient reflects a change in intensity near the minutia point, divided by the distance over that intensity change. Though the calculations are relatively simple, they are able to approximate the two properties around minutiae seen by a forensic examiner. 5.1.2 Contrast Gradient Quality Measurement Illustration To illustrate this measurement, we first look at the gradient values on a clear print with well defined minutiae. Figure 5.1 shows such a print. Along with it are close-up views of three minutiae from the print, where the ridge ending and bifurcations are clearly seen. One would expect these very clear minutiae to be at the high end of the quality scale when measuring latent minutiae quality. They are at pixel locations (97, 100), (126, 167), and (111, 68), where the origin (0,0) is the upper left corner of the image. The largest gradients at these 3 locations are 94.0, 66.5, and 88.0 intensity units per grid unit. Looking at gradient values in each \\(5 \\times 5\\) neighborhood, we find slightly higher gradient values for each minutia: 107.0, 122.0, and 121.0. For each minutia, we shift our focus point to the new \\((x_0, y_0)\\) corresponding to the slightly higher gradient value. For these \\(n_3(x_0, y_0)\\) neighborhoods, we find the largest intensity differences to get the contrast. Contrast measures for the three locations are, respectively, 0.447, 0.651, and 0.561, yielding quality metrics 107.0 \\(\\times\\) 0.447 = 47.84, 122.0 \\(\\times\\) 0.651 = 79.42, and 121.0 \\(\\times\\) 0.561 = 67.85. The quality metric ranges from 0.133-100.0 (see Figure 5.5) for NIST’s SD27A latent fingerprint database, which has 15,008 defined minutia locations. A quality metric value cannot exceed 100.0, and any larger scores are capped at 100.0 (out of the 15,008 minutia, only 54 produced quality metrics over 100.0). We now turn to the issue of applying this quality metric to determine the usability of minutiae information in a latent print. Figure 5.1: (Figure 1 from Peskin and Kafadar (n.d.).) Example of a clear fingerprint and close-up views of three minutiae from this print. Each horizontal green line in the whole print ends at one of the minutia. The high quality minutiae above are contrasted with those in a latent print. Previously available database NIST SD27A included latent prints classified as “good,” “bad,” and “ugly” by forensic examiners to which we apply our quality metric. Figure 5.2 shows three typical latent fingerprints from this data set, one from each class. Figure 5.3 shows a closer look at one of the highest quality minutiae on each of these three latent fingerprints. Figure 5.2: (Figure 2 from Peskin and Kafadar (n.d.).) Examples of a good, bad, and ugly latent print from the SD27A set: G043, B132, and U296. Figure 5.3: (Figure 3 from Peskin and Kafadar (n.d.)). Examples of good, bad, and ugly minutiae of approximately the same quality. G043: (502, 741), quality = 21.9, B132: (553, 651), quality = 27.3, and U296: (548, 655), quality = 24.3. Each minutia is located at the center of the 40 × 40 cropped image. To understand how our quality measure compares with an examiner’s assessment, we select one of the latent prints and show minutiae that were selected by one examiner at different quality levels. Figure 5.4 shows a “bad” latent and close-up views of three minutiae with quality scores 5.0, 15.2, and 28.8. The gradient and contrast measures increase from left to right in these \\(20 \\times 20\\) pixel images, and the quality metric is calculated within the center quarter (\\(5 \\times 5\\)) of the image. Figure 5.4: (Figure 4 from Peskin and Kafadar (n.d.)). One of the fingerprints labeled as bad, B110, and an expanded view of three of the minutiae from one examiner, in order of quality: (742, 903), quality = 5.0; (727, 741), quality = 15.2; and (890, 405), quality = 28.8. For each minutiae point, the examiner-marked location is at the center of a 20 × 20 square of pixels. The SD27A set of latent prints is associated with 849 sets of forensic examiner data, each an ANSI/NIST formatted record, containing the locations of all marked minutiae. From these, we can compare our algorithm’s quality metric with the examiner’s assessment of “good,” “bad,” or “ugly.” In Figure 5.5, we tabulate the number of minutiae that a forensic examiner located in each set, and the average quality metric of all minutiae in each set. In addition to calculating the average quality metric across all minutiae on the print, we also calculate the average for only those subsets of minutiae with quality metric exceeding 10.0, 20.0, and 30.0. In each set of results, latent prints labeled “good” have more higher-scoring minutiae than prints labeled “bad,” and substantially more than prints labeled “ugly.” The average minutiae quality metric for each set is similar, suggesting that the assigned label of “good,” “bad,” or “ugly” is highly influenced by the number of distinguishable (high-scoring) minutiae on the print. Figure 5.5: (Table 1 from Peskin and Kafadar (n.d.)) Numbers of minutiae and average minutiae quality metric (\\(Q\\)) for three sets of minutia for: all sets combined, and subsets defined by \\(Q\\) (SD = standard deviation). We then compared our minutiae quality metric to the ridge quality map, which is provided in record 9.308 of the American National Standard for Information Systems data format for the Interchange of Fingerprint, Facial, and other Biometric Information (NIST, n.d.). The 9.308 record contains a small grid of scores for individual pixels on small sections of the latent print, ranked for the quality of a ridge present in that section, with 5 representing the highest score and 0 the lowest. From these grids of 0-5 values, we obtained the ridge quality scores for individual minutia locations. We compare our (objective) quality metric scores with observer (subjective) ridge qualities for all 15,008 minutia in the database, as shown in Table 5.1 Table 5.1: Means and standard deviations (SD) of quality metric values for all 15,008 prints by their ordinal ridge quality score. Ridge Quality Score Frequency Quality Metric Mean Quality Metric SD 1 76 18.6 19.9 2 10829 23.7 17.1 3 3940 26.5 18.2 4 144 43.2 26.1 5 17 39.9 14.8 5.1.3 Test for Usable Quality We designed a procedure to identify a threshold for this quality metric, below which the feature is unreliable, and above which it may provide reliable information for comparison purposes. To identify this threshold value, clear fingerprint images are systematically degraded and quality metric scores calculated for the minutiae accompanying each degraded image. We start by recognizing that a typical clear print has foreground (ridge) intensity values of 255 (black) on a scale of 0-255 (white-black). Accordingly, we simulate different levels of image quality by decreasing the quality of clear prints by lowering the foreground intensity levels to levels lower than 255. As the foreground quality decreases, contrast between the minutiae and background also decreases. In this way, we create a series of images from each clear print with different levels of minutiae quality. We can then ask experts to evaluate which minutiae are, in their judgments, sufficiently distinguishable to be useful in a fingerprint analysis, and then note their conclusions following an actual comparison (“correct match found” or “incorrect match found”). This way, a range of \\(Q\\) for minutiae that are highly correlated with accuracy of analysis can be estimated. Note that one cannot use actual latent prints for such a study, because the background on latent prints is not well characterized and ground truth is unknown. Figure 5.6 shows an example of a clear print with foreground (ridges) starting at 255 followed by a series of prints in which the ridge intensity is progressively lowered to 100. Figure 5.7 magnifies the region around three of the minutiae for foreground values equal to 255 and 100 to show the decreased visibility of the minutiae when the gradients and contrast are severely reduced. Figure 5.6: (Figure 6 from Peskin and Kafadar (n.d.)). A clear fingerprint on the left and a series of transformations with ridge intensity lowered from 255 to 220, 200, 180, 160, 140, 120, and 100. Figure 5.7: (Figure 7 from Peskin and Kafadar (n.d.)). Three of the minutiae from Figure 5.6 with foreground = 255 (top row) and foreground = 100 (bottom row). Given a quality metric and a method for systematically decreasing the quality of a fingerprint, we can now design an experiment with different examiners of different abilities and correlate the results of their analyses with true outcomes. If accuracy exceeds, say, 95% only when the print has at least \\(n_0\\) minutiae having quality metrics above a threshold \\(Q_0\\), then the examiner has an objective criterion for the “A” (analysis) phase of the ACE-V process. 5.2 Data Figure 5.8: Magnetic fingerprint powder clinging to a rod. Source: Stapleton and Associates A common method for recovering latent prints is dusting. Moisture clinging powder is gently brushed onto a surface using soft brushes to increase the visibility of fingerprints. After dusting, prints are usually recorded by lifting – placing a piece of transparent tape over the fingerprint then transferring the tape, along with the powder, onto a card of contrasting color. Photographs may also be taken of the powdered print. In place of dusting, prints may be chemically processed (super glue, ninhydrin, Rhodamine 6G (R6G), etc.). Clearly visible prints (patent prints) such as those made by paint or blood may be photographed directly. If the fingerprint is left on an object that is easily transported, the object should be sent to the forensics lab and photographed to create a digital image in a controlled environment. Figure 5.9: Examples of powdered and lifted fingerprints using powders of different color. Source: Minsei Matec Co. Photographs and fingerprint cards are scanned or otherwise converted to digital format. Enhancements to contrast or color may be made using a photo editing software before producing a finalized grayscale image that may be entered into an AFIS database, directly compared to a suspects’ exemplar prints (known fingerprints, e.g., those taken on a ten print card at a police station), or run through a quality metric algorithm. The information contained in a fingerprint can be divided into three levels (Scientific Working Group on Friction Ridge Analysis and Technology 2011). Level 1 detail is the most general, consisting of overall pattern type, friction ridge flow, and morphological information. While insufficient for individualization, level 1 detail may be used for exclusion. “Fingerprint pattern classification” refers to overall fingerprint patterns (e.g., loop, whorl) and their subcategories (e.g., left/right slanted) (Hicklin et al. 2011). Figure 5.10: Level 1 detail in a fingerprint includes overall pattern type and ridge flow, such as loops, whorls, and arches. Source: Duquesne University Level 2 detail consists of minutiae and individual friction ridge paths. Minutiae are also called “Galton details” after Sir Francis Galton, the first to define and name specific minutiae (bifurcation, enclosure, ridge endings, island) (Galton 1892). Figure 5.11: Different types of Level 2 detail, or minutiae, in a fingerprint. Source: syhrl.blogspot.com. Level 3 detail is the most specific, including friction ridge dimensional attributes such as width, edge shapes, and pores. These details may or may not appear on an exemplar print and are the least reliable. Figure 5.12: Level 3 detail of a fingerprint. Source: onin.com/fp/. 5.2.1 ACE-V and AFIS Latent prints may be submitted to an AFIS database which will return the top n potential matches. Examiners can then perform the ACE-V comparison process on these prints until a “match” is found: Analysis: a digitized latent print is analyzed to determine if it is “suitable” for examination. During this process, latent print examiners (LPEs) mark clear, high quality sections of a print in green, moderate quality sections in yellow, and unclear or distorted sections in red. These colors correspond to features that can be used in comparison, may possibly be used, and will likely not be useful for comparison, respectfully. As level 3 detail is unreliable, LPEs use level 1 and 2 detail to determine if a print is suitable to continue to the comparison step. If not, the ACE-V process ends here. Comparison: the latent and exemplar prints are compared side by side. In addition to overall pattern and ridge flow, examiners may look for the existence of target groups – unique clusters of minutiae – that correspond between a latent and exemplar. Additional features may be marked in orange. Evaluation: a decision of identification (formerly “individualization”), exclusion, or inconclusive is made based on OSAC standards. An inconclusive conclusion may be reached if either print does not contain enough information for a decision. Examiners may request consultation with a colleague before reaching a decision, who would perform an independent markup on the latent print. Verification: a decision may (or may not) be verified by another examiner who performs an independent markup of the latent print. If the second examiner does not know the first examiner’s decision, it is considered a blind verification. IAFIS is the Integrated Automatic Fingerprint Identification System developed and maintained by the US FBI (Investigation, n.d.a). Implemented in 1999, it contains criminal history, photographs, and fingerprints for over 70 million individuals with criminal histories and fingerprints of 34 million civilians (Investigation, n.d.a, @fbi_ngi). The FBI’s Next Generation Identification (NGI) System was announced in 2014 to extend and improve the capabilities of IAFIS. INTERPOL also maintains a database of over 181,000 fingerprint records and almost 11,000 latent prints (INTERPOL, n.d.). NIST maintains a series of Biometric Special Databases and Software (NIST, n.d.), including several fingerprint databases. Special Database 302, not yet released, will contain realistic latent prints and their corresponding exemplars collected from the Intelligence Advanced Research Projects Activity (IARPA) Nail to Nail (N2N) Fingerprint Challenge (Fiumara 2018). Annotations to these latent prints may be released at a future date. Special Database 27A, which has since been withdrawn, contained 258 latent and rolled mate pairs that two or more LPEs have agreed “match” (i.e. print pairs not guaranteed to be ground truth matches) (Watson 2015). The latent prints in this database were classified into three categories: good, bad, and ugly, which allows for general testing of correspondence between overall fingerprint quality and quality scores. 5.3 R Package The R Package fingerprintr implements the Contrast Gradient Quality Measurement for quantifying the quality of individual features, or minutiae, in a latent fingerprint (Kafadar and Pan 2019). The primary functions include reading in the fingerprint image into the correct format (convert_image) then calculating the quality scores (quality_scores). If desired, additional information on gradient and contrast can be output by setting verbose = TRUE in the quality_scores function. If one wishes to only see gradient and contrast values, these can be output by functions find_maxgrad and find_contrast, respectively. library(bmp) library(fingerprintr) # this image is the first of two used in the simple latent case study below temp_image &lt;- read.bmp(&quot;simple_latent.bmp&quot;) # minutiae information (one per row without semicolons): X,Y; 72,22; 72,32; # 35,80; 90,85; 59,144 temp_min &lt;- read.csv(&quot;simple_latent_min.txt&quot;, header = TRUE, sep = &quot;,&quot;) image_file &lt;- convert_image(temp_image, &quot;bmp&quot;) min_file &lt;- as.matrix(temp_min) # quality scores quality_scores(image_file, min_file) quality_scores(image_file, min_file, verbose = TRUE) # if image already in pixel array format, must be transposed before running # quality_scores text_image &lt;- read.csv(&quot;simple_latent.txt&quot;, header = FALSE, sep = &quot;\\t&quot;) quality_scores(t(text_image), min_file) 5.4 Drawing Conclusions Presently, the first step in fingerprint comparison is the analysis phase, in which the examiner assesses a print for usable minutia that are judged to be sufficiently clear and distinctive for comparison with prints in a database. To reduce the subjectivity in this assessment, we have proposed a “minutia quality metric” for assessing the clarity, and hence usability, of minutia (e.g., ridge endings, bifurcations, islands) in a fingerprint. The metric is scaled between 0 (totally unusable) and 100 (perfectly clear); more high-scoring minutia should lead to greater distinctiveness in the print and hence fewer false positives that can occur when trying to match latent prints to database prints using much lower-quality minutia. We have shown in this chapter that our metric is both computationally efficient and correlates well with image quality: by systematically (via image algorithms) reducing the image quality of a print (and hence of the minutia), the quality metric decreases accordingly. We also show, using NIST SD27A fingerprint images, that the existence of more high-quality minutia correlates well with the experts’ three-category assessment of fingerprint images (good, bad, ugly). In future work, we plan to evaluate the value of this algorithm for real practice, and estimate false positive and false negative rates with and without the quality metric. We report on this work in a forthcoming article. 5.5 Case Study Figure 5.13: Five simple fingerprints on a clean glass plate. Five simple fingerprints were created on a clean glass plate using a single finger with differing levels of pressure. These levels range from one to five, with one indicating the largest amount of pressure applied. The glass plate was dusted using a black magnetic powder and the revealed prints lifted with tape onto a white fingerprint card. As expected, as pressure decreases, the latent prints decrease in overall quality (visually, the ridges are less thick and lose some continuity) and amount of friction ridge area captured. The entire fingerprint card was scanned and each print was cropped into its own grayscale image file. These cropped images were converted into their equivalent 2D pixel array values (0 to 255, black to white). Five features (ridge endings (E) and bifurcations (B)) from the second and third prints were analyzed using the Contrast Gradient Quality Metric. As the metric is interested in contrast and gradients, the darker, blacker ridges in the Figure 5.14 receive overall higher scores than the lighter gray ridges in the Figure 5.15 (feature 3 is an exception). .twoC {width: 80%} .clearer {clear: both} .twoC .table {max-width: 40%; float: right} .twoC img {max-width: 40%; float: left; margin-top:50px;} Figure 5.14: Second print Feature Type Score 1 E 70.27 2 B 57.76 3 E 54.99 4 E 58.32 5 B 54.78 Figure 5.15: Third print Feature Type Score 1 E 29.46 2 B 19.99 3 E 79.86 4 E 20.97 5 B 23.79 Although only five features are marked in the latent prints above, many more would be identified in casework. In cases where large number of minutiae are identified, minutiae quality scores could allow examiners to focus first on those with higher quality, from which presumably the most reliable information may be obtained. The minutiae in the first image above are relatively clear even to an inexperienced observer, and clearly of better contrast than minutiae in the second image, which the quality scores reflect. Using the quality scores, examiners may be able to focus on high scoring features first, or ignore minutiae scoring below a certain threshold. Acknowledgements We would like to thank Dr. Adele Peskin for vital discussion and conversation leading to the creation and development of the Contrast Gradient Algorithm as well as an unpublished manuscript (Peskin and Kafadar, n.d.). References "],
["shoe.html", "Chapter 6 Shoe Outsole Impression Evidence 6.1 Introduction 6.2 Data 6.3 R Package(s) 6.4 Drawing Conclusions 6.5 Case Study", " Chapter 6 Shoe Outsole Impression Evidence Soyoung Park, Sam Tyner 6.1 Introduction In the mess of a crime scene, one of the most abundant pieces of evidence is a shoe outsole impression (Bodziak 2017). A shoe outsole impression is the trace of a shoe that is left behind when the shoe comes in contact with walking surface. Shoe outsole impressions are often left in pliable materials such as sand, dirt, snow, or blood. Crime scene impressions are lifted using adhesive or electrostatic lifting, or casting to obtain the print left behind. When a shoe outsole impression is found at a crime scene, the question of interest is, “Which shoe left that impression?” Suppose we have a database of shoe outsole impressions. We want to find the close images in the database to the questioned shoe impression to determine the shoe brand, size, and other class characteristics. Alternatively, if we have information about potential suspects’ shoes, then we need to investigate whether the questioned shoe impressions share characteristics. The summary statistic we need is the degree of correspondence between the questioned shoe outsole impression from the crime scene (\\(Q\\)) and the known shoeprint from a database or a suspect (\\(K\\)). If the similarity between \\(Q\\) and \\(K\\) is high enough, then we may conclude that the source for \\(Q\\) and \\(K\\) is the same. Thus, the goal is to quantify the degree of correspondence between two shoe outsole impressions. Note that \\(Q\\) and \\(K\\) have different origins for shoe than they do for trace glass evidence: for glass evidence, sample \\(Q\\) comes from a suspect and \\(K\\) comes from the crime scene, while for shoe outsole impression evidence, sample \\(Q\\) comes from the crime scene and sample \\(K\\) comes from a suspect or a database. 6.1.1 Sources of variability in shoe outsole impressions There are many characteristics of shoes that are examined. First, the size of the shoe is determined, then the style and manufacturer. These characteristics are known as class characteristics: there are large numbers of shoes that share these characteristics, most other shoes that do not share class characteristics with the impression are easily excluded. For instance, a very popular shoe in the United States is the Nike Air Force one, pictured below (Smith 2009). So, seeing a Nike logo and concentric circles in an impression from a Men’s size 13 instantly excludes all shoes that are not Nike Air Force Ones in Men’s size 13. Figure 6.1: The outsole of a Nike Air Force One Shoe. This pattern is common across all shoes in the Air Force One model. Source: nike.com Next, subclass characteristics are examined. These characteristics are shared by a subset of elements in a class, but not by all elements in the class. In shoe impressions, subclass characteristics usually occur during the manufacturing process. For instance, air bubbles may form in one manufacturing run but not in another. Also, the different molds used to create the same style and size shoes can have slight differences, such as where pattern elements intersect (Bodziak 1986). Just like with class characteristics, subclass characteristics can be used to eliminate possible shoes very easily. Finally, the most unique parts of a shoe outsole impression are the randomly acquired characteristics (RACs) left behind. The RACs are smaller knicks, gouges, and debris in soles of shoes that are acquired over time, seemingly at random, as the shoes are worn. These are the identifying characteristics of shoe impressions, and examiners look for these irregularities in the crime scene impressions to make an identification. 6.1.2 Current practice Footwear examiners compare \\(Q\\) and \\(K\\) by considering class, subclass and identifying characteristics on two impressions. The guideline for comparing footwear impressions from the Scientific Working Group for Shoeprint and Tire Tread Evidence (SWGTREAD), details seven possible conclusions for comparing \\(Q\\) and \\(K\\) (Standard for Terminology Used for Forensic Footwear and Tire Impression Evidence 2013): Lacks sufficient detail (comparison not possible) Exclusion Indications of non-association Limited association of class characteristics Association of class characteristics High degree of association Identification Examiners rely on their knowledge, experience, and guidelines from SWGTREAD to come to one of these seven conclusions for footwear impression examinations. 6.1.3 Goal of this chapter In this chapter, we will show our algorithmic approach to measure the degree of similarity between two shoe outsole impression. For this, CSAFE collected shoe outsole impression data, developed the method for calculating similarity score between two shoe impressions and R package shoeprintr for the developed method (Park and Carriquiry 2019b). 6.2 Data Crime scene data that we can utilize to develop and test a comparison algorithm for footwear comparison is very limited for several reasons: Shoe impressions found at crime scenes are confidential because they are a part of cases involving real people. Although some real, anonymized crime scene data is available, we typically don’t know the true source of the impression. Most shoe outsole impressions found at the crime scene are partial, degraded, or otherwise imperfect. They are not appropriate to develop and test an algorithm. 6.2.1 Data collection CSAFE collected shoe impressions using the two-dimensional EverOS footwear scanner from Evident, Inc.. The scanner is designed to scan the shoe outsole as the shoe wearer steps onto the scanner. As more pressure is put on the scanner, there will be more detailed patterns detected from the outsole. The resulting images have resolution of 300 DPI. In addition, the images show a ruler to allow analysts to measure the size of the scanned impression. Figure 6.2 shows examples of impression images from the EverOS scanner. On the left, there are two replicates of the left shoe from one shoe style, while on the right, there are two replicates from the right shoe from a different shoe stle. The repeated impressions are very similar, but not exact because there are some differences in the amount and location of pressure from the shoe wearer while scanning. Figure 6.2: Examples of images from the EverOS scanner. At left, two replicates of one left shoe. At right, two replicates from the right shoe of another pair of shoes. This scanner enables us to collect shoe impressions and make comparisons where ground truth is known. By collecting repeated impressions from the same shoe, we can construct comparisons between known mates (same shoe) and known non-mates (different shoes). CSAFE has made available a large collection of shoe outsole impressions from the EverOS scanner (CSAFE 2019). 6.2.2 Transformation of shoe image The resulting image from the EverOS scanner is very large, making comparisons of images very time-consuming. To speed up the comparison process, we used MATLAB to downsample all of the images at 20%. Next, we apply edge detection to the images, which leaves us with outlines of important patterns of shoe outsoles. We extract the edges of the outsole impression using the Prewitt operator to obtain the (\\(x\\),\\(y\\)) coordinates of the shoe patterns. All class and subclass characteristics, as well as possible RACs on the outsole impression are retained using this method. The final data we use for comparison are the \\(x\\) and \\(y\\) coordinate values of the impression area edges that we extracted from the original image in the \\(Q\\) shoe and the \\(K\\) shoe, as shown in Figure 6.3. shoeQ &lt;- read.csv(&quot;dat/Q_03L_01.csv&quot;) shoeK &lt;- read.csv(&quot;dat/K_03L_05.csv&quot;) shoeQ$source &lt;- &quot;Q&quot; shoeK$source &lt;- &quot;K&quot; library(tidyverse) bind_rows(shoeQ, shoeK) %&gt;% ggplot() + geom_point(aes(x = x, y = y), size = 0.05) + facet_wrap(~source, scales = &quot;free&quot;) Figure 6.3: Outsole impressions of two shoes, \\(K\\) and \\(Q\\). 6.2.3 Definition of classes We use the data CSAFE collected to construct pairs of known mates (KM) and known non-mates (KNM) for comparison. The KMs are two repeated impressions from the same shoe, while the KNMs are from two different shoes. For the known non-mates, the shoes are identical in size, style, and brand, but they come from two different pairs of shoes worn by two different people. We want to train a comparison method using this data because shoes that are the same size, style, and brand are the most similar to each other, and thus the detection of differences will be very hard. By training a method on the hardest comparisons, we make all comparisons stronger. 6.3 R Package(s) The R package shoeprintr was created to perform shoe outsole comparisons (Chapter 3 in Park (2018)). To begin, the package can be installed from GitHub using devtools: devtools::install_github(&quot;CSAFE-ISU/shoeprintr&quot;, force = TRUE) We then attach it and other packages below: library(shoeprintr) library(patchwork) We use the following method to quantify the similarity between two impressions: Select “interesting” sub-areas in the \\(Q\\) impression found at the crime scene. Find the closest corresponding sub-areas in the \\(K\\) impression. Overlay sub-areas in \\(Q\\) with the closest corresponding areas in \\(K\\). Define similarity features we can measure to create an outsole signature. Combine those features into one single score. To begin our comparison, we choose three circular sub-areas of interest in \\(Q\\) by giving the information of their centers and their radii. These areas can be selected “by hand” by examiners, as we do here, or by computer, as we do in Section 6.5. The three circles of interest are given in input_circles input_circ &lt;- matrix(c(75.25, 110, 170, 600.4, 150, 470, 50, 50, 50), nrow = 3, ncol = 3) input_circ ## [,1] [,2] [,3] ## [1,] 75.25 600.4 50 ## [2,] 110.00 150.0 50 ## [3,] 170.00 470.0 50 For circle \\(q_1\\), we select the center of (75.25, 600.4) and the radius of 50. The second and the third rows in this matrix show center and radius for circle \\(q_2, q_3\\). start_plot(shoeQ[, -3], shoeK[, -3], input_circ) Here, we can draw the graph of coordinates of edges from \\(Q\\) and \\(K\\). For \\(Q\\), we use the function start_plot draw the \\(Q\\) impressions colored by the three circular areas we chose. We call the red circle, \\(q_1\\), orange \\(q_2\\), and green \\(q_3\\). The goal here is to find the closest areas to \\(q_1\\), \\(q_2\\), and \\(q_3\\) in shoe \\(K\\). To find the closest areas of \\(q_1\\), \\(q_2\\), and \\(q_3\\) in shoe \\(K\\), we use the function match_print_subarea. In this particular comparisons with full impressions of \\(Q\\) and \\(K\\), we know that circle \\(q_1\\) is located in left toe area of \\(Q\\). Thus, the algorithm confines the searching area in \\(K\\) into upper left area of \\(K\\). match_print_subarea(shoeQ, shoeK, input_circ) Figure 6.4: Final match result between shoe Q and shoe K The function match_print_subarea will produce the best matching areas in shoe \\(K\\) for circles of \\(q_1\\), \\(q_2\\), and \\(q_3\\) in shoe \\(Q\\), which we fixed. Figure 6.4 shows the final result from the function match_print_subarea. Three circles of red, orange, green colors in the right panel in Figure 6.4 indicate that those circles that the algorithm found showed the best overlap with circles of \\(q_1\\), \\(q_2\\), and \\(q_3\\) in shoe \\(Q\\). Next, we explore how the function match_print_subarea finds the corresponding areas between two shoe impressions. Too find the corresponding areas for circle \\(q_1\\) in shoe \\(K\\), the underlying algorithm first finds many candidate circles in shoe \\(K\\), as shown in Figure 6.5. Figure 6.5: Circle \\(q_11\\) in \\(Q\\) (left) and candidate circles in \\(K\\) For the comparison, the function selects many candidate circles in \\(K\\) labeled as circle \\(k_1, \\dots, k_9\\) in Figure 6.5. For candidate circles in \\(K\\), we use larger radius than circle \\(q_1\\) because we want any candidate circles to fully contain circle \\(q_1\\) if they are mates. Ideally, the union of the candidate circles in \\(K\\) should contain all edge points in \\(K\\). The algorithm compares all candidate circles in shoe \\(K\\) with the fixed circle \\(q_1\\)and picks the closest one as the area corresponding to \\(q_1\\) in shoe \\(K\\). 6.3.1 One subarea matching In this section, we show one example of the matching process by examining the comparison between circle \\(q_1\\) and circle \\(k_8\\) in Figure 6.5. We selected \\(k_8\\) because it is a close match to circle \\(q_1\\) well. nseg = 360 circle_q1 &lt;- data.frame(int_inside_center(data.frame(shoeQ), 50, nseg, 75.25, 600.4)) circle_k8 &lt;- data.frame(int_inside_center(data.frame(shoeK), 65, nseg, 49, 700)) match_q1_k8 &lt;- boosted_clique(circle_q1, circle_k8, ncross_in_bins = 30, xbins_in = 20, ncross_in_bin_size = 1, ncross_ref_bins = NULL, xbins_ref = 30, ncross_ref_bin_size = NULL, eps = 0.75, seed = 1, num_cores = parallel::detectCores(), plot = TRUE, verbose = FALSE, cl = NULL) The shoeprinter function boosted_clique finds the subset of pixels (the maximal clique) that can be used for alignment of circle \\(q_1\\) and circle \\(k_8\\). Using corresponding pixels, the function computes the rotation angle and translation metric which result the best alignment between them. Circle \\(q_1\\) is transformed to be on top of the circle \\(k_8\\), using the calculated alignment information. The function then produces summary plots as shown in Figure 6.6 and a table of similarity features as shown in Table 6.1. Figure 6.6: Resulting plot when comparing circle \\(q_1\\) (blue points) and circle \\(k_8\\) (red points). Figure 6.6 has four sections. In the first row, first column, the distances between all points in the maximal clique are shown: the x-axis is the distance between points in circle \\(k_8\\) and the y-axis is the distance between points in circle \\(q_1\\). These values should fall on or near the \\(y=x\\) diagonal, because for identical circles, the points in the maximal clique are the same. The second plot in the first row of Figure 6.6 shows the \\((x,y)\\) values of the points in the maximal clique. Red points are from \\(k_8\\), blue circles are from \\(q_1\\). The bottom two plots show all points in the two circles after alignment. We can see that the two circles share a large area of overlap: the blue points are almost perfectly overlapping with the red points. Table 6.1: Resulting table from the matching between q1 and k8 Clique size Rot. angle Overlap on k8 Overlap on q1 Median distance center x center y radius 18 12.05 0.75 0.97 0.3 54.5 688.5 50.28 Table 6.1 contains the similarity measures and other information about \\(k_8\\), the closest matching circle to \\(q_1\\). The first five features measure the degree of similarity between circle \\(q_1\\) and circle \\(k_8\\), after aligning them. There are 18 pixels in the maximal clique, and the rotation angle between them is 12.05\\(^o\\). The two overlap features indicate that 75% of circle \\(k_8\\) pixels are overlapped with circle \\(q_1\\) and 97% of circle \\(q_1\\) pixels are overlapped with circle \\(k_8\\) after alignment. The median value of the distance between two close points after aligning the circles is 0.3. For the clique size and overlap metrics, larger values indicate more similar circles, while the opposite is true for distance metrics. The last three columns are about information of the found circle in \\(K\\) that is matched the best with circle \\(q_1\\). Thus, the center (54.5, 688.5) and the radius 50.28 in \\(K\\) is the best matching circle to \\(q_1\\). Figure 6.7: Circles \\(q_1\\) and \\(k_8\\) in context. Figure 6.7 shows the two matching circles as part of the larger shoe impression. By fixing circle \\(q_1\\) (in blue), we found the closest circle (in red) in shoe \\(K\\). The blue circle in \\(Q\\) and red circle in \\(K\\) look pretty similar. This process would be repeated for the two other circles that we fixed in \\(Q\\), using the function match_print_subarea. 6.4 Drawing Conclusions To determine whether impressions \\(Q\\) and \\(K\\) were made by the same shoe or not, we need to define the signature of the questioned shoe outsole impression. We define the signature of shoe \\(Q\\) as the three circular areas of the shoe after edge detection. Figure 6.8: Comparing the lengths of the blue lines is important for determining if the same shoe made impression \\(Q\\) and impression \\(K\\). Figure 6.8 shows the signature in \\(Q\\) and the corresponding areas in \\(K\\) found by the match_print_subarea function. We use the average of the five similarity features from the three circle matchings to draw a conclusion. If the circles in \\(K\\) match the circle in \\(Q\\), then the corresponding geometry between the three circles in \\(Q\\) and \\(K\\) should be very similar. The blue lines in Figure 6.8 form a triangle whose vertices are the centers of the three circles in each shoe impression. The difference of the side lengths (numbered 1,2,3 in the figure) between the two triangles are an additional feature used to determine if \\(Q\\) and \\(K\\) were made my the same shoe or not. If the two triangles in Figure 6.8 have similar side lengths, this is evidence that shoe \\(Q\\) and shoe \\(K\\) are from the same source. Table 6.2: Resulting table comparing Q and K. Comparison Clique size Rot. angle Overlap on k* Overlap on q Median distance Diff in length from Triangle \\(q_1\\)-\\(k^*_1\\) 18 12.05 0.75 0.97 0.3 0.58 \\(q_2\\)-\\(k^*_2\\) 17 10.57 0.53 0.91 0.43 0.55 \\(q_3\\)-\\(k^*_3\\) 20 12.14 0.63 1 0.24 1.03 Table 6.2 contains the summary features from comparing the signature with three circles in \\(Q\\) to the corresponding areas in \\(K\\) which are the closest match. There is one row for each comparison between \\(q_i\\) and \\(k^{*}_i\\) for \\(i=1,2,3\\). The last column is the absolute value of difference of the triangle side lengths from three circles in \\(Q\\) and \\(K\\). Smaller differences indicate more similarity between \\(Q\\) and \\(K\\). Finally, we take the average of all features except rotation angle over the three circle. For the rotation angle, we take the standard deviation of the three measurements instead of the mean. If the two prints come from the same shoe, the rotation angle will be very similar in all three circles, so we expect small values of the standard deviation to indicate mates, while large values indicate non-mates. 6.5 Case Study Comparing two impressions with totally different outsole patterns is a very easy problem for humans and computers. But what if we are tasked to compare two impressions from two different shoes from different people that have the same brand, model, and size? Suppose we have one questioned shoe outsole impression (\\(Q\\)) and two known shoe impressions (\\(K_1, K_2\\)) from two different shoes. All three impressions share the same class characteristics. How can we conclude if the source of \\(Q\\) is the same as \\(K_1\\) or \\(K_2\\)? Figure 6.9: Example images of shoe outsole impressions. The questioned impression (\\(Q\\)) and two known impressions (\\(K_1\\), \\(K_2\\)). Does \\(Q\\) have the same source as \\(K_1\\) or \\(K_2\\)? Figure 6.9 displays three outsole impressions. With a cursory glance, it looks like impression \\(Q\\) could have been made by the same shoe as either \\(K_1\\) or \\(K_2\\). The two known impressions are from the same brand and model of shoes, which were worn by two different people for about six months. Since the three impressions all share class and subclass characteristics, it is a very hard comparison. There are some differences among images \\(Q, K_1, K_2\\) but it is hard to determine if these differences are due to measurement errors or variation in the EverOS scanner or to wear and tear of the outsole or RACs. 6.5.1 Compare \\(Q\\) and \\(K_1\\) Let’s compare \\(Q\\) and \\(K_1\\) first. In \\(Q\\), we select three circular areas. For this, we use the function initial_circle to select three centers within the (30%, 80%), (20%, 40%), (70%, 70%) quantiles of the \\((x, y)\\) ranges of the coordinates in \\(Q\\). The function automatically generates corresponding centers of the circles with radius of 50. input_circles_Q &lt;- initial_circle(imgQ) input_circles_Q ## [,1] [,2] [,3] ## [1,] 89.7 578.2 50 ## [2,] 67.8 296.6 50 ## [3,] 177.3 507.8 50 start_plot(imgQ, imgK1, input_circles_Q) Figure 6.10: The input circles and the shoe \\(Q\\) impression (left) and the shoe \\(K_1\\) impression (right). Next, run match_print_subarea() to find circles in \\(K\\) that most closely correspond to the circles in \\(Q\\). The result is shown in Figure 6.11. In \\(K_1\\), the algorithm finds the closest circles which show the most overlap with each of the circles that we fixed in \\(Q\\). The corresponding circle information and summary features are shown in Tables 6.3 and 6.4, respectively. Each row in Table 6.4 shows the similarity features from circle \\(q_i\\) and circle \\(k^{1}_i\\) when \\(i=1,2,3\\). match_print_subarea(imgQ, imgK1, input_circles_Q) Figure 6.11: The input circles (\\(q_1, q_2, q_3\\)) and the shoe \\(Q\\) impression (left) and the closest matching circles (\\(k_1^1, k_2^1, k_3^1\\)) and shoe \\(K_1\\) impression (right), which are the result of match_print_subarea. Table 6.3: Centers and radius information for circles in \\(K_1\\) that are the closest match to the circles in \\(Q\\) according to the matching algorithm. Reference_X Reference_Y Reference_radius 71.0 567.5 52.28 85.5 286.0 56.32 182.5 495.0 54.15 Table 6.4: Similarity features from comparing circles in \\(Q\\) to the best matching circles in \\(K_1\\). clique_size rotation_angle reference_overlap input_overlap med_dist_euc diff 15 1.23 0.55 0.86 0.49 0.58 18 1.88 0.42 1.00 0.09 20.62 17 2.68 0.65 0.96 0.31 7.49 6.5.2 Compare \\(Q\\) and \\(K_2\\) We repeat the process from Section 6.5.1 to find the best matching circles in \\(K_2\\). match_print_subarea(imgQ, imgK2, input_circles_Q) Figure 6.12: The input circles (\\(q_1, q_2, q_3\\)) and the shoe \\(Q\\) impression (left) and the closest matching circles (\\(k_1^2, k_2^2, k_3^2\\)) and shoe \\(K_2\\) impression (right), which are the result of match_print_subarea. Figure 6.12 shows the circles from the matching algorithm. In this comparison, \\(k_3^2\\) is in a very different position compared to \\(q_{3}\\). Table 6.5: Centers and radius information that the matching algorithm found as the best overlap circle in \\(K_2\\) for fixed circles in \\(Q\\) Reference_X Reference_Y Reference_radius 66.0 582.5 51.20 66.0 302.0 55.61 199.5 431.0 51.44 Table 6.6: Similarity features by comparing circles in \\(Q\\) to the best matching circles in \\(K_2\\). clique_size rotation_angle reference_overlap input_overlap med_dist_euc diff 18 1.87 0.64 0.89 0.53 1.95 14 7.16 0.39 0.92 0.52 89.54 16 32.93 0.26 0.51 0.58 52.26 Table 6.5 and Table 6.5 show the location information of matching circles in \\(K_2\\) for fixed circles in \\(Q\\) and their corresponding similarity features, respectively. 6.5.3 Interpreting Comparisons between \\(Q, K_1\\) and \\(Q, K_2\\) To summarize similarity features from the match of the two impressions, we will take the average of each similarity feature across the three circle matchings, and the standard deviation of the rotation angle estimates. For features such as clique size and overlap, larger values indicate more similar impressions. For the features median distance and absolute value of differences of lengths from two triangles in two impressions, smaller values indicate more similar impressions. For the rotation angle estimation, we use the standard deviation of the three values because we are interested in comparing the three values to each other, not in the value of the angles themselves. If all rotations are about the same, the algorithm is consistently finding those patterns, and the prints are likely from the same shoe, even if the image has been rotated . If the rotation angles are very different, however, the algorithm is forcing similarity where it doesn’t exist, and the prints are likely from different shoes. Table 6.7: Summary table of two comparisons of \\(Q-K_1\\) and \\(Q-K_2\\). Similarity features are averaged but rotation angles are summarized as standard deviation. Match clique_size sd_rot_angle overlap_k overlap_q med_dist diff_length Q, \\(K_1\\) 16.67 0.73 0.54 0.94 0.30 9.56 Q,\\(K_2\\) 16.00 16.62 0.43 0.77 0.54 47.92 Table 6.7 shows the summarized similarity features from two comparisons we did for \\(Q,K_1\\) and \\(Q, K_2\\). In both comparisons, the average clique size and median distance look similar. The standard deviation (SD) of rotation angle estimations and difference in lengths from triangles, however, are very different. The comparison \\(Q,K_1\\) results in similar rotation estimates (1.23, 1.88, 2.68), while the comparison \\(Q,K_2\\) results in very different rotation estimates (1.87, 7.16, 32.93) in Table 6.4 and Table 6.6. The length comparison from \\(Q,K_2\\) is five times larger than that from the \\(Q,K_1\\) comparison. In addition, \\(Q,K_1\\) has high overlap average chance of about 94% while \\(Q-K_2\\) has about a 77% average. Therefore, we have evidence that prints \\(Q\\) and \\(K_1\\) were left by the same shoe, while prints \\(Q\\) and \\(K_2\\) were left by different shoes. Finally, we reveal the truth: the impressions \\(Q\\) and \\(K_1\\) are from the same shoe. The impression \\(K_2\\) is scanned from different shoe of the same size, brand, and style. Because the two shoes share class characteristics, the comparisons we did here were the hardest cases. To see what typical values we will have for mates and non-mates, we need to do more analyses to get similarity features from many comparisons. In other analyses, for instance in Park (2018), we use random forests to classify mates and non-mates using the definitions of clique size, overlap, etc. as the features. The empirical probability from the random forest could serve as a score, similar to what was done in Chapter 7. References "],
["glass.html", "Chapter 7 Trace glass evidence: chemical composition 7.1 Introduction 7.2 Data 7.3 R Packages 7.4 Drawing Conclusions 7.5 Case Study", " Chapter 7 Trace glass evidence: chemical composition Soyoung Park, Sam Tyner 7.1 Introduction It is easy to imagine a crime scene with glass fragments: a burglar may have broken a glass door, a glass bottle could have been used in an assault, or a domestic disturbance may involve throwing something through a window. During the commission of a crime, there are many ways that glass can break and be transferred from the scene. The study of glass fragments is important to forensic science because the glass broken at the scene can transfer to the perpetrator’s shoes, clothing, or even their hair (Curran, Hicks, and Buckleton 2000). Crime scene investigators collect fragments of glass at the scene as a part of the evidence collection process, and the fragments are sent to the forensic science lab for processing. Similarly, evidence such as clothing and shoes are collected from a suspect, and if glass is found, the fragments are sent to the lab and compared to the fragments found at the scene. The question that the analyst usually tries to answer is, “Did these glass fragments come from the same source?” This is a source level question, meaning that the comparison of the fragments will only tell the investigators whether or not the fragments from the suspect and the fragments from the scene have the same origin. As discussed in Section 1.3, the forensic analysis will not inform investigators how the suspect came into contact (activity level) with the glass or if the suspect was the perpetrator of the crime (offense level) (Roger Cook et al. 1998). 7.1.1 Problems of interest There are two key problems of interest in glass fragments comparison, but before defining them, we need to define the different glass involved in the investigation of a crime. Glass fragments found on the suspect, for example in their hair, shoes, or clothes, are questioned fragments, which we denote by \\(Q\\). Glass fragments found at the crime scene, for example in front of a broken window or taken from the broken window, are known fragments, which we denote \\(K\\). This brings us to a specific source question: Did the questioned fragments \\(Q\\) found on the suspect come from the same source of glass as the known fragments \\(K\\), which we know belong to a specific piece of glass at the scene? The goal is now to quantify the similarity between \\(Q\\) and \\(K\\). There are lots of ways measure similarity between two glass fragments, but the metric should be defined according to available databases of glass fragment measurements for which ground truth is known. For example, if we have elemental compositions measured in parts per million (ppm) as numerical values, the similarity can be quantified by the difference of the chemical compositions of \\(Q\\) and \\(K\\). 7.1.2 Current practice There are many types of glass measurements such as color, thickness, refractive index (RI) and chemical concentrations. In this chapter, we will focus on float glass that is most frequently used in windows, doors and automobiles. For discussions of the other measurements see e.g. Curran, Hicks, and Buckleton (2000). The elemental concentrations of float glass that we use here were obtained through inductively coupled mass spectrometry with a laser add-on (LA-ICP-MS). In the current practice, there are two analysis guides from ASTM International, (ASTM-E2330-12 2012) and (ASTM-E2927-16 2016). To determine the source of glass fragments according to these two guides, intervals around the mean concentrations are computed for each element, and if all elements’ intervals overlap, then the conclusion is that the fragments come from the same source. For more detail on these methods, see ASTM-E2330-12 (2012) and ASTM-E2927-16 (2016). 7.1.3 Comparing glass fragments In order to determine if two glass fragments come from the same source, a forensic analyst considers many properties of the glass, including color, fluorescence, thickness, surface features, curvature, and chemical composition. All methods for examining these properties, except for methods of chemical composition analysis, are non-destructive. If the fragments are large, exclusion are easy to reach if the glass are of different colors because of the wide variety of glass colors possible in manufacturing. Typically, however, glass fragments are quite small and color determination is very difficult. Similarly, thickness of glass is dictated by the manufacturing process, which aims for uniform thickness, so if two glass fragments differ in thickness by more than 0.25mm, an exclusion is made (Bottrell 2009). For glass fragments of the same color and thickness, microscopic techniques for determining light absorption (fluorescence), curvature, surface features (such as coatings), are used before the destructive chemical composition analysis. 7.1.4 Goal of this chapter In this chapter, we construct a new rule for making glass source conclusions using the random forest algorithm to classify the source of glass fragments (Park and Carriquiry 2019a). 7.2 Data 7.2.1 Chemical composition of glass The process for determining the chemical composition of a glass fragment is given in great detail in ASTM-E2330-12 (2012) and ASTM-E2927-16 (2016). This destructive method determines elemental composition with Inductively Coupled Plasma Mass Spectrometry (ICP-MS). Up to 40 elements can be detected in a glass fragment using this method. In Weis et al. (2011), only 18 elements are used: calcium (Ca), sodium (NA) and magnesium (Mg) are the major elements, followed by aluminum (Al), potassium (K) and iron (Fe) as minor elements, and lithium (Li), titanium (Ti), manganese (Mn), rubidium (Rb), strontium (Sr), zirconium (Zr), barium (Ba), lanthanum (La), cerium (Ce), neodymium (Nd), hafnium (Hf), and lead (Pb) as the trace elements. The methods of Weis et al. (2011) use standard deviations (\\(\\sigma\\)) of repeated measurements of the same fragment to create intervals around the measurements. Intervals of width \\(2\\sigma, 4\\sigma, 6\\sigma, 8\\sigma, 10\\sigma, 12\\sigma, 16\\sigma, 20\\sigma, 30\\sigma,\\) and \\(40\\sigma\\) are considered for overlap. 7.2.2 Data source Dr. Alicia Carriquiry of Iowa State University commissioned the collection of a large database of chemical compositions of float glass samples. The details of this database are explained in Park and Carriquiry (2019a). The full database is available here. The database includes 31 panes of float glass manufactured by Company A and 17 panes manufactured by Company B, both located in the United States. The Company A panes are labeled AA, AB, … , AAR, and the Company B panes are labeled BA, BB, … ,BR. The panes from Company A were produced during a three week period (January 3-24, 2017) and the panes from Company B were produced during a two week period (December 5-16, 2016). To understand variability within a ribbon of glass, two glass panes were collected on almost all days in each company, one from the left side and one from the right side of the ribbon. Twenty four fragments were randomly sampled from each glass pane. Five replicate measurements were obtained for 21 of the 24 fragments in each pane; for the remaining three fragments in each pane, we obtained 20 replicate measurements. Therefore, each pane has 165 measurements for 18 elements. For example, see the heuristic in Figure 7.1. In some panes, there may be a fragment with fewer than five replicate measurements. The unit for all measurements is parts per million (ppm). library(tidyverse) pane &lt;- expand.grid(x = 1:16, y = 1:4) n &lt;- nrow(pane) pane$id &lt;- 1:n frags &lt;- sample(n, 24) rep20 &lt;- sample(frags, 3) rep5 &lt;- frags[!(frags %in% rep20)] pane_sample &lt;- data.frame(frag = c(rep(rep5, each = 5), rep(rep20, each = 20)), rep = c(rep(1:5, 21), rep(1:20, 3))) sample_data &lt;- left_join(pane, pane_sample, by = c(id = &quot;frag&quot;)) ggplot(data = sample_data, aes(x = x, y = y)) + geom_tile(fill = &quot;white&quot;, color = &quot;black&quot;) + geom_jitter(aes(color = as.factor(rep)), alpha = 0.8, size = 0.5) + scale_color_manual(values = c(rep(&quot;black&quot;, 20))) + theme_void() + theme(legend.position = &quot;none&quot;) Figure 7.1: An example of how the glass fragments were sampled, if the 64 squares are imagined to be randomly broken fragments within a pane. 7.2.3 Data structure Next, we look at the glass data. glass &lt;- read.csv(&quot;dat/glass_raw_all.csv&quot;) head(glass) ## pane fragment Rep mfr element ppm ## 1 AA 1 1 A Al 2678.000 ## 2 AA 1 1 A Ba 10.800 ## 3 AA 1 1 A Ca 63140.000 ## 4 AA 1 1 A Ce 9.520 ## 5 AA 1 1 A Fe 667.000 ## 6 AA 1 1 A Hf 1.148 The elements have very different scales, as some (e.g. Ca) are major elements, some (e.g. Al) are minor elements, and others (e.g Hf) are trace elements. Thus, we take the natural log transformation of all measurements to put them on a similar scale. # need to make sure the panes are shown in order of mfr date pane_order &lt;- c(paste0(&quot;A&quot;, LETTERS[c(1:13, 15, 22:25)]), paste0(&quot;AA&quot;, LETTERS[c(1:4, 6, 8:13, 17:18)]), names(table(glass$pane))[c(32:48)]) glass$pane &lt;- ordered(glass$pane, levels = pane_order) glass_log &lt;- glass %&gt;% mutate(log_ppm = log(ppm)) glass_log %&gt;% select(-ppm) %&gt;% spread(element, log_ppm) %&gt;% select(mfr, pane, fragment, Rep, Li, Na, Mg, Al, K, Ca) %&gt;% head() ## mfr pane fragment Rep Li Na Mg Al K ## 1 A AA 1 1 0.8329091 11.54025 10.04715 7.892826 7.136483 ## 2 A AA 1 2 0.5423243 11.53772 10.03671 7.883069 7.126248 ## 3 A AA 1 3 0.7227060 11.53126 10.04499 7.909122 7.127694 ## 4 A AA 1 4 0.7975072 11.54219 10.05449 7.912423 7.144407 ## 5 A AA 1 5 0.7227060 11.52968 10.02260 7.871311 7.103322 ## 6 A AA 2 1 0.5596158 11.52663 10.04107 7.898411 7.118826 ## Ca ## 1 11.05311 ## 2 11.04930 ## 3 11.04580 ## 4 11.06147 ## 5 11.01023 ## 6 11.03747 cols &lt;- csafethemes:::csafe_cols_secondary[c(3, 12)] glass_log %&gt;% filter(element %in% c(&quot;Li&quot;, &quot;Na&quot;, &quot;Mg&quot;, &quot;Al&quot;, &quot;K&quot;, &quot;Ca&quot;)) %&gt;% ggplot() + geom_density(aes(x = log_ppm, fill = mfr), alpha = 0.7) + scale_fill_manual(name = &quot;Manufacturer&quot;, values = cols) + facet_wrap(~element, scales = &quot;free&quot;, nrow = 2) + labs(x = &quot;Log concentration (ppm)&quot;, y = &quot;Density&quot;) + theme(legend.position = &quot;top&quot;) Figure 7.2: Density estimation of selected chemical compositions, colored by manufacturers Figure 7.2 shows density plots of six chemical compositions of Al, Ca, K, Li, Mg, Na. Na and Ca (major elements) show density curves from two manufacturers are overlapped, while Al, K show clear separation between curves by manufacturers. This implies that glass fragments from different manufacturers will be very easy to distinguish. glass_log %&gt;% filter(element %in% c(&quot;Na&quot;, &quot;Ti&quot;, &quot;Zr&quot;, &quot;Hf&quot;)) %&gt;% ggplot() + geom_boxplot(aes(x = pane, y = log_ppm, fill = mfr), alpha = 0.8, outlier.size = 0.5, size = 0.1) + scale_fill_manual(name = &quot;Manufacturer&quot;, values = cols) + facet_wrap(~element, scales = &quot;free&quot;, nrow = 2, labeller = label_both) + theme_bw() + theme(legend.position = &quot;none&quot;) + scale_x_discrete(labels = c(&quot;AA&quot;, rep(&quot;&quot;, 30), &quot;BA&quot;, rep(&quot;&quot;, 15), &quot;BR&quot;)) + labs(x = &quot;Pane (in order of manufacture)&quot;, y = &quot;Log concentration (ppm)&quot;) Figure 7.3: Box plot of four elements in 48 panes, ordered by date of production, from two manufacturers. Figure @ref(fig:box plot) shows box plots of the measurements of four elements (Na, Ti, Zr, Hf) in each of the 48 panes in the database, colored by manufacturer. Boxes are ordered by date of production within manufacturer. We can confirm the between- and within- pane variability. Interestingly, element values of Zr and Hf in manufacturer A are both decreasing in time, which is evidence that the element measurements are highly correlated. To account for this relationship, we use methods that do not require independence of measurements. col_data_AA &lt;- glass_log_long[, -6] %&gt;% filter(pane == &quot;AA&quot;) %&gt;% spread(element, log_ppm) %&gt;% group_by(fragment) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) col_data_BA &lt;- glass_log_long[, -6] %&gt;% filter(pane == &quot;BA&quot;) %&gt;% spread(element, log_ppm) %&gt;% group_by(fragment) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) P1 &lt;- ggcorr(col_data_AA[, 3:20], geom = &quot;blank&quot;, label = TRUE, hjust = 0.75) + geom_point(size = 10, aes(color = coefficient &gt; 0, alpha = abs(coefficient) &gt; 0.5)) + scale_alpha_manual(values = c(`TRUE` = 0.25, `FALSE` = 0)) + guides(color = FALSE, alpha = FALSE) P2 &lt;- ggcorr(col_data_BA[, 3:20], geom = &quot;blank&quot;, label = TRUE, hjust = 0.75) + geom_point(size = 10, aes(color = coefficient &gt; 0, alpha = abs(coefficient) &gt; 0.5)) + scale_alpha_manual(values = c(`TRUE` = 0.25, `FALSE` = 0)) + guides(color = FALSE, alpha = FALSE) P1 + P2 7.3 R Packages We propose a machine learning method to quantify the similarity between two glass fragments \\(Q\\) and \\(K\\). The goal here is to construct a classifier that predicts, with low error, whether or not \\(Q\\) and \\(K\\) have the same source. Using the glass database, we construct many pairwise comparisons between two glass fragments with known sources, and we will record their source as either same source or different source. To construct the set of comparisons, we Take the natural log of all measurements of all glass fragments. (ppm to \\(\\log\\)(ppm)) Select one pane of glass in the database to be the “questioned” source. Sample one fragment from this pane, and call it \\(Q\\). Select one pane of glass in the database to be the “known” source. Sample one fragment from this pane, and call it \\(K\\). Construct the response variable: if the panes in 2 and 3 are the same, the response variable is KM for known mates. Otherwise, the response variable is KNM for known non-mates. Construct the features: take the mean of the repeated measurements of \\(Q\\) and \\(K\\) in each element, and take the absolute value of the difference between the mean of \\(Q\\) and the mean of \\(K\\). Repeat 2-5 until we have a data set suitable for training a classifier. We use the R package caret to train a random forest classifier to determine whether two glass fragments (\\(Q\\) and \\(K\\)) have the same source or have different sources (Jed Wing et al. 2018). To begin, the package can be installed from CRAN: install.packages(&quot;caret&quot;) library(caret) # other package used for plotting library(GGally) library(patchwork) The R package caret (Classification And REgression Training) is used for applied predictive modeling. The caret package allows us to run 238 different models as well as adjusting data splitting, preprocessing, feature selection, tuning parameter, and variable importance estimation. We use it here for fitting a random forest model to our data using cross-validation and down-sampling. diff_Q_K_data &lt;- readRDS(&quot;dat/rf_data_kfrags_1z.rds&quot;) Table 7.1: Differences of log values of concentrations (Li, Na, Mg, Al, K, Ca) from pairs of known mates (KM) and known non-mates (KNM) Class Li Na Mg Al K Ca KM 0.0078 0.0043 0.0089 0.0270 0.0103 0.0149 KM 0.0845 0.0065 0.0042 0.0049 0.0126 0.0032 KM 0.0826 0.0170 0.0069 0.0137 0.0259 0.0014 KNM 0.5362 0.0581 0.0267 0.0741 0.0834 0.0619 KNM 0.2437 0.0283 0.0092 0.0181 0.0377 0.0386 KNM 0.2319 0.0316 0.0283 0.0001 0.0437 0.0518 Table 7.1 shows the example of pairwise differences among glass measurements. If we take the difference of two fragments from the same pane, then KM is assigned to the response variable of Class. If we take the difference of two fragments from two different panes, then KNM is assigned to variable Class. Each row has 18 differences and one variable Class indicating the source of two glass fragments. By taking pairwise differences, there are many more KNM pairs than KM pairs: we can construct 67,680 KNM pairs but only 1,440 KM pairs from the glass database. diff_Q_K_data %&gt;% gather(element, diff, Li:Pb) %&gt;% filter(element %in% c(&quot;Zr&quot;, &quot;Li&quot;, &quot;Hf&quot;, &quot;Ca&quot;)) %&gt;% ggplot() + geom_density(aes(x = diff, fill = class), alpha = 0.7) + scale_fill_manual(name = &quot;Class&quot;, values = cols) + facet_wrap(~element, scales = &quot;free&quot;, nrow = 2) + labs(x = &quot;Difference between Q and K (log(ppm))&quot;) + theme(legend.position = &quot;top&quot;) Figure 7.4: Histogram of differences from four chemical elements among KM and KNM. Figure 7.4 shows the distribution of differences for KM and KNM pairs in the Ca, Hf, Li, and Zr. Across all elements, distribution of differences from KNM pairs are more dispersed than differences of KM. The differences of KM have high density near zero, while the KNM measurements are shifted to the right and have a long tail. These differences for all 18 elements will be the features of the random forest. Finally, we construct the random forest classifier. Since the data have 1,440 KM and 67,680 KNM observations, the response variable is imbalanced. With this large imbalance, the algorithm can simply predict KNM and have low error rate without learning anything about the properties of the KM class. You can find more ways to consider this imbalance problem for fitting the random forest in this glass fragment source prediction in (Park and Carriquiry 2019a). In this chapter, we will down-sample the KNM observations to equal the number of KM observations. That way, we will have 1,440 KM and 1,440 KNM comparisons. Then, we sample 70% of them to be the training set and the remaining 30% are the testing set. diff_Q_K_data$class &lt;- as.factor(diff_Q_K_data$class) # Down sample the majority class to the same number of minority class set.seed(123789) down_diff_Q_K_data &lt;- downSample(diff_Q_K_data[, c(1:18)], diff_Q_K_data$class) down_diff_Q_K_data &lt;- down_diff_Q_K_data %&gt;% mutate(id = row_number()) names(down_diff_Q_K_data)[19] &lt;- &quot;class&quot; table(down_diff_Q_K_data$class) ## ## KM KNM ## 1440 1440 # Create training set train_data &lt;- down_diff_Q_K_data %&gt;% sample_frac(0.7) # Create test set test_data &lt;- anti_join(down_diff_Q_K_data, train_data, by = &quot;id&quot;) train_data &lt;- train_data[, -20] #exclude id test_data &lt;- test_data[, -20] #exclude id # dim(train_data) # 2016 19 dim(test_data) # 864 19 After down-sampling and setting aside 30% of the data for testing, we can train the classifier. Below is the R code to fit the random forest, using caret (Jed Wing et al. 2018). The tuning parameter for the random forest algorithm is mtry, the number of variables available for splitting at each tree node. We use five values (1, 2, 3, 4, 5) to tune mtry and pick the optimal one. For the classification, the default mtry is the square root of the number of predictor variables (it is \\(\\sqrt{18} \\approx 4.2\\), in our study). To pick the optimal mtry parameter, the area under the receiver operating characteristic (ROC) curve is calculated. The optimal value will result the highest area under the ROC curve (AUC). The data are also automatically centered and scaled for each predictor. We use 10-fold cross-valildation to evaluate the random forest algorithm and repeat entire process three times. ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 3, savePredictions = &quot;final&quot;, summaryFunction = twoClassSummary, classProbs = TRUE) # mtry is recommended to use sqrt(# of variables)=sqrt(18) so 1:5 are tried # to find the optimal one RF_classifier &lt;- train(class ~ ., train_data, method = &quot;rf&quot;, tuneGrid = expand.grid(.mtry = c(1:5)), metric = &quot;ROC&quot;, preProc = c(&quot;center&quot;, &quot;scale&quot;), trControl = ctrl) RF_classifier &lt;- readRDS(&quot;dat/RF_classifier.RDS&quot;) RF_classifier ## Random Forest ## ## 2016 samples ## 18 predictor ## 2 classes: &#39;KM&#39;, &#39;KNM&#39; ## ## Pre-processing: centered (18), scaled (18) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1815, 1814, 1814, 1814, 1815, 1814, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 1 0.9637918 0.9826797 0.8323266 ## 2 0.9641196 0.9807190 0.8467239 ## 3 0.9625203 0.9758170 0.8457239 ## 4 0.9624018 0.9738562 0.8467172 ## 5 0.9614865 0.9728758 0.8473838 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 2. From the RF fitting result, it showed the AUC (ROC), Sens (Sensitivity) and Spec (Specificity) values in each mtry values. R-package caret selects the best mtry value (it is 2, in our study) to give the highest AUC (ROC) value 0.964, from 10-fold cross-validation and 3 repeated process. The final performance of the random forest algorithm with the optimal mtry of 2 with 500 of number of trees. In the training set, the false negative rate is 0.02 and the false positive rate is 0.153 of false positive rate, which is the optimized classification result. We see the OOB estimate of error rate of 0.085, which is the average error rate from the fold left out of the training set during cross-validation. RF_classifier$finalModel$confusion ## KM KNM class.error ## KM 1000 20 0.01960784 ## KNM 152 844 0.15261044 imp &lt;- varImp(RF_classifier)$importance imp &lt;- as.data.frame(imp) imp$varnames &lt;- rownames(imp) # row names to column rownames(imp) &lt;- NULL imp &lt;- imp %&gt;% arrange(-Overall) imp$varnames &lt;- as.character(imp$varnames) elements &lt;- read_csv(&quot;dat/elements.csv&quot;) imp %&gt;% left_join(elements[, c(2, 4)], by = c(varnames = &quot;symb&quot;)) %&gt;% ggplot(aes(x = reorder(varnames, Overall), y = Overall, fill = classification)) + geom_bar(stat = &quot;identity&quot;, color = &quot;grey40&quot;) + scale_fill_brewer(name = &quot;Element&quot;, palette = &quot;Blues&quot;, direction = -1) + labs(y = &quot;Overall importance&quot;, x = &quot;Variable&quot;) + scale_y_continuous(position = &quot;right&quot;) + coord_flip() + theme_bw() Figure 7.5: Variable importance from the RF classifier, colored by element types (major, minor or trace). By fitting the random forest algorithm, we can also get a measure of variable importance. This metric ranks which elements out of 18 are most important to correctly predicting the source of the glass fragments. Figure 7.5 shows that elements K, Ce, Zr, Rb, and Hf are five very important variables and Pb, Sr, Na, Ca, and Mg are five minimally important variables. All major elements are not important: this is not surprising because all glass is, broadly speaking, very similar chemically. Conversely, most of the trace elements are ranked in the top half. 7.4 Drawing Conclusions The result of the random forest classifier is a probability of the observation belonging to the two classes, KM and KNM, and a predicted class. For an observation, if the class probability for KM is greater than 0.5, then the class prediction is M (mated), otherwise it is NM (non-mated). We use the random forest classifier to predict on the test set. Recall that we know the ground truth, KM and KNM for the test data. # Get prediction from RF classifier in test data pred_class &lt;- predict(RF_classifier, test_data) table_pred_class &lt;- confusionMatrix(pred_class, test_data$class)$table table2 &lt;- data.frame(table_pred_class) %&gt;% spread(Reference, Freq) table2$Prediction &lt;- c(&quot;M&quot;, &quot;NM&quot;) table2 %&gt;% knitr::kable(format = &quot;html&quot;, caption = &quot;Classification result of test set&quot;, longtable = FALSE, row.names = FALSE) Table 7.2: Classification result of test set Prediction KM KNM M 409 69 NM 11 375 Table 7.2 shows the classification results on the testing set from the random forest. There are 420 KM and 444 KNM comparisons in the test set. For the KM cases, the RF classifier correctly predicts the source as M in 97.4% of cases and wrongly predicts the source as NM in 2.6% of cases. This is the false negative rate, FNR. For the KNM cases, the RF correctly classifies them as NM 84.5% of the time, while 15.5% are incorrectly classified as M. We know that the method resulted in higher FPR (15.5) because the data we have are from many close panes made by the same manufacturer that are difficult to distinguish from one another. # class probability for the same pane is used as similarity score for RF prob_prediction &lt;- predict(RF_classifier, test_data, type = &quot;prob&quot;)[, &quot;KM&quot;] test_data$prob &lt;- prob_prediction test_data %&gt;% ggplot() + geom_density(aes(x = prob, fill = class), alpha = 0.7) + scale_fill_manual(name = &quot;Class&quot;, values = cols) + labs(x = &quot;Empirical probability for KM from RF classifier&quot;, y = &quot;Density&quot;) + theme(legend.position = &quot;top&quot;) Figure 7.6: Scores from the RF classifier for the test set. Ground truth (either KM or KNM) is known. Any observations above 0.5 are declared same source, while all others are declared different source. Notice the long tail in the KNM cases. Figure 7.6 shows the distribution of the RF scores, colored by true classes in test data. The score is the empirical class probability that an observation belongs to the KM class. The modes of the the two classes are well separated, while there is still some overlap between the classes’ density curve. The tail of the distribution of scores for KNMs is more skewed than that of the KMs, which helps explain the higher false positive rate. Table 7.2 is the predicted classification result using the cut-off of 0.5. If the RF score is larger than 0.5, then we will predict the pair of glass fragments have the same source (M). If not, then we declare they have different source (NM). 7.5 Case Study Here we introduce a new set of five pairs of comparisons and use the random forest classifier we trained in Section @ref(glass_rpkgs) to determine if the two samples being compared are from the same source of glass or from different sources. Suppose you are given the following data on eight glass samples, where each sample has been measured 5 times: Your supervisor wants you to make five comparisons: compare_id sample_id_1 sample_id_2 1 1 2 2 1 3 3 1 5 4 4 6 5 7 8 First, you need to take the log of the measurements, then get the mean for each sample and each element. # take the log, then the mean of each for comparison new_samples &lt;- mutate(new_samples, logppm = log(ppm)) %&gt;% select(-ppm) %&gt;% group_by(sample_id, element) %&gt;% summarise(mean_logppm = mean(logppm)) Next, make each of the five comparisons. We write a function, make_compare() to do so, then we use purrr::map2() to perform the comparison for each row in compare_id. The resulting compared data is below. make_compare &lt;- function(id1, id2) { dat1 &lt;- new_samples %&gt;% filter(sample_id == id1) %&gt;% ungroup() %&gt;% select(element, mean_logppm) dat2 &lt;- new_samples %&gt;% filter(sample_id == id2) %&gt;% ungroup() %&gt;% ungroup() %&gt;% select(element, mean_logppm) tibble(element = dat1$element, diff = abs(dat1$mean_logppm - dat2$mean_logppm)) %&gt;% spread(element, diff) } compared &lt;- comparisons %&gt;% mutate(compare = map2(sample_id_1, sample_id_2, make_compare)) %&gt;% unnest() DT::datatable(compared) %&gt;% DT::formatRound(4:21, 3) Finally, we take the compared data and predict using the random forest object: newpred &lt;- predict(RF_classifier, newdata = compared, type = &quot;prob&quot;) # only take the id data from compare, and the M prediction from newpred bind_cols(compared[, 1:3], score = newpred[, 1]) %&gt;% mutate(prediction = ifelse(score &gt; 0.5, &quot;M&quot;, &quot;NM&quot;)) %&gt;% knitr::kable(format = &quot;html&quot;, caption = &quot;Predicted scores of new comparison cases from the random forest classifier&quot;, longtable = FALSE, row.names = FALSE, digits = 3, col.names = c(&quot;Comparison ID&quot;, &quot;Sample ID 1&quot;, &quot;Sample ID 2&quot;, &quot;Score&quot;, &quot;Decision&quot;)) Table 7.3: Predicted scores of new comparison cases from the random forest classifier Comparison ID Sample ID 1 Sample ID 2 Score Decision 1 1 2 0.812 M 2 1 3 0.238 NM 3 1 5 0.002 NM 4 4 6 0.430 NM 5 7 8 0.618 M Table 7.3 shows the RF score which is defined as the empirical class probability that the samples are from the same pane (M) by the RF algorithm6. Based on these scores, it appears that samples 1 and 2 are very likely from the same source, while samples 1 and 5 are very likely from different sources. Using a threshold of 0.5, sample 4 and sample 6 are predicted to be from different sources. However, this score is so close to the threshold of 0.5 that we may want to say this is inconclusive.7 Samples 1 and 3 are probably from different sources, though we are less confident in that determination than we are in the decision that samples 1 and 5 are from different sources. Finally, samples 7 and 8 are probably from the same source, but we are less certain of this than we are that samples 1 and 2 are from the same source. More research into the appropriateness of this random forest method for making glass source conclusions is needed. Important points to consider are: What database should be used to train the algorithm? Could the random forest method over fit, reducing the generalizability of this method? Nevertheless, the RF classifier makes good use of the glass fragment data, with its high dimension and small number of repeated measurements. More details on the RF classifier and additional discussion can be found in Park and Carriquiry (2019a). References "],
["decision-making.html", "Chapter 8 Decision-making in Forensic Identification Tasks 8.1 Introduction 8.2 Data 8.3 R Packages 8.4 Drawing Conclusions 8.5 Case Study", " Chapter 8 Decision-making in Forensic Identification Tasks Amanda Luby 8.1 Introduction Although forensic measurement and analysis tools are increasingly accurate and objective, many final decisions are largely left to individual examiners (PCAST 2016). Human decision-makers will continue to play a central role in forensic science for the foreseeable future, and it is unrealistic to assume that, within the United States’ current criminal justice system, there are no differences in the decision-making process between examiners, day-to-day forensic decision-making tasks are equally difficult, or human decision-making can be removed from the process entirely. The role of human decisions in forensic science is perhaps most studied in the fingerprint domain, which will be the focus of this chapter. High-profile examples of misidentification have inspired studies showing that fingerprint examiners, like all humans, may be susceptible to biased instructions and unreliable in final decisions (Dror and Rosenthal 2008) or influenced by external factors or contextual information (Dror, Charlton, and Péron 2006; Dror and Cole 2010). These studies contradict common perceptions of the accuracy of fingerprint examination, and demonstrate that fingerprint analysis is far from error-free. Although fingerprint examination is the focus of this chapter, it is not the only forensic domain that relies on human decision-making. Firearms examination (see, e.g., NRC (2009b) pg. 150-155) is similar to latent print examination in many ways, particularly in that examiners rely on pattern evidence to determine whether two cartridges originated from the same source. Handwriting comparison (see National Research Council (2009b) pg. 163-167 on “Questioned Document Examination” and Stoel et al. (2010) for discussion) consists of examiners determining whether two samples of handwriting were authored by the same person, taking potential forgery or disguise into account. A third example is interpreting mixtures of DNA evidence (see PCAST (2016) Section 5.2). A DNA mixture is a biological sample that contains DNA from two or more donors and requires analysts to make subjective decisions to determine how many individuals contributed to the DNA profile. Due to these currently unavoidable human factors, the President’s Council of Advisors on Science and Technology (2016) recommended increased “black box” error rate studies for these and other subjective forensic science methods. The FBI “Black Box” study (Bradford T. Ulery et al. 2011) was the first large-scale study performed to assess the accuracy and reliability of latent print examiners’ decisions. The questions included a range of attributes and quality seen in casework, and were representative of searches from an automated fingerprint identification system. The overall false positive rate in the study was 0.1% and the overall false negative rate was 7.5%. These computed quantities, however, have excluded all “inconclusive” responses (i.e. neither identifications nor exclusions). This is noteworthy, as nearly a third of all responses were inconclusive and respondents varied on how often they reported inconclusives. Respondents who report a large number of inconclusives, and only make identification or exclusion decisions for the most pristine prints, will likely make far fewer false positive and false negative decisions than respondents who reported fewer inconclusives. The authors of the study also note that it is difficult to compare the error rates and inconclusive rates of individual examiners because each examiner saw a different set of fingerprint images (see Appendix 3 of Bradford T. Ulery et al. (2011)). In other words, it would be unfair to compare the error rate of someone who was given a set of “easy” questions to the error rate of someone who was given a set of “difficult” questions. A better measure of examiner skill would account for both error rates and difficulty of prints that were examined. Accurately measuring proficiency, or examiner skill, is valuable not only for determining whether a forensic examiner has met baseline competency requirements, but for training purposes as well. Personalized feedback after participating in a study could lead to targeted training for examiners in order to improve their proficiency. Additionally, if proficiency is not accounted for among a group of study participants, which often include trainees or non-experts as well as experienced examiners, the overall results from the study may be biased. There also exist substantial differences in the difficulty of forensic evaluation tasks. Properties of the evidence, such as the quality, quantity, concentration, or rarity of characteristics may make it easier or harder to evaluate. Some evidence, regardless of how skilled the examiner is, will not have enough information to result in an identification or exclusion in a comparison task. An inconclusive response, in this case, should be treated as the “correct” response. Inconclusive responses on more straightforward identification tasks, on the other hand, may be treated as mistakes. Methods for analyzing forensic decision-making data should thus provide estimates for both participant proficiency and evidence difficulty. Item response models, a class of statistical methods used prominently in educational testing, have been proposed for use in forensic science for these reasons (Kerkhoff et al. 2015). Luby and Kadane (2018) provided the first item response analysis for forensic proficiency test data, and we improve and extend upon that work by - analyzing a different fingerprint identification study that includes richer data on decision-making, and - extending the range of models considered. The remainder of the chapter is organized as follows: Section 8.1.1 gives a brief overview of Item Response Models, Section 8.2 provides an overview on how decision-making data is collected in forensic science, and Section 8.3 describes an R package that can be used to fit these models. Section 8.4 describes how conclusions are drawn from an Item Response analysis, and Section 8.5 gives an example IRT analysis of the FBI “Black Box” study. 8.1.1 A Brief Overview of Item Response Models For \\(P\\) individuals responding to \\(I\\) test items, we can express the binary responses (i.e. correct/incorrect) as a \\(P\\times I\\) matrix, \\(Y\\). Item Response Theory (IRT) is based on the idea that the probability of a correct response depends on individual proficiency, \\(\\theta_p, p = 1, \\ldots, P\\), and item difficulty, \\(b_i, i = 1, \\ldots I\\). 8.1.1.1 Rasch Model The Rasch Model (Rasch 1960; Fischer and Molenaar 2012) is a relatively simple yet powerful item response model, and serves as the basis for extensions introduced later. The probability of a correct response is modeled as a logistic function of the difference between the participant proficiency, \\(\\theta_p\\) (\\(p=1, \\dots, P\\)), and the item difficulty, \\(b_i\\) (\\(i=1, \\dots, I\\)): \\[\\begin{equation} P(Y_{pi} = 1) = \\frac{1}{1-\\exp(-(\\theta_p - b_i))}. \\tag{8.1} \\end{equation}\\] To identify the model, we shall use the convention of constraining the mean of the participant parameters (\\(\\mu_\\theta\\)) to be equal to zero. This allows for a nice interpretation of both participant and item parameters relative to the “average participant”. If \\(\\theta_p &gt;0\\), participant \\(p\\) is of “above average” proficiency and if \\(\\theta_p &lt;0\\), participant \\(p\\) is of “below average” proficiency. Similarly, if \\(b_i &lt; 0\\) question \\(i\\) is an “easier” question and the average participant is more likely to correctly answer question \\(i\\). If \\(b_i &gt;0\\) then question \\(i\\) is a more “difficult” question and the average participant is less likely to correctly answer question \\(i\\). Other common conventions for identifying the model include setting a particular \\(b_i\\) or the mean of the \\(b_i\\)s equal to zero. The item characteristic curve (ICC) describes the relationship between proficiency and performance on a particular item (see Figure 8.1 for examples). For item parameters estimated under a Rasch model, all ICCs are standard logistic curves with different locations on the latent difficulty/proficiency scale. Note that Equation (8.1) also describes a generalized linear model (GLM), where \\(\\theta_p - b_i\\) is the linear component, with a logit link function. By formulating the Rasch Model as a hierarchical GLM with prior distributions on both \\(\\theta_p\\) and \\(b_i\\), the identifiability problem is solved. We assign \\(\\theta_p \\sim N(0, \\sigma_\\theta^2)\\) and \\(b_i \\sim N(\\mu_b, \\sigma_b^2)\\), although more complicated prior distributions are certainly possible. The two-parameter logistic model (2PL) and three-parameter logistic model (3PL) are additional popular item response models (Lord 1980). They are both similar to the Rasch model in that the probability of a correct response depends on participant proficiency and item difficulty, but additional item parameters are also included. We omit a full discussion of these models here, but further reading may be found in van der Linden and Hambleton (2013) and Boeck and Wilson (2004). Figure 8.1: Item Characteristic Curve (ICC) examples for the Rasch, 2PL, and 3PL models. 8.1.1.2 Partial Credit Model The partial credit model (PCM) (Masters 1982) is distinct from the models discussed above because it allows for the response variable, \\(Y_{pi}\\), to take additional values beyond zero (incorrect) and one (correct). This is especially useful for modeling partially correct responses, although may be applied in other contexts where the responses can be ordered. When \\(Y_{pi}\\) is binary, the partial credit model is equivalent to the Rasch model. Under the PCM, the probability of response \\(Y_{pi}\\) depends on \\(\\theta_p\\), the proficiency of participant \\(p\\) as in the above models; \\(m_i\\), the maximum score for item \\(i\\) (and the number of step parameters); and \\(\\beta_{il}\\), the \\(l^{th}\\) step parameter for item \\(i\\) (\\(l=0, \\dots, m_i\\)): \\[\\begin{equation} P(Y_{pi} = 0) = \\frac{1}{1+\\sum_{k=1}^{m_i} \\exp \\sum_{l=1}^k (\\theta_p - \\beta_{il})} \\end{equation}\\] \\[\\begin{equation} P(Y_{pi} = y, y&gt;0) = \\frac{\\exp\\sum_{l=1}^y(\\theta_p - \\beta_{il})}{1+\\sum_{k=1}^{m_i}\\exp \\sum_{l=1}^k (\\theta_p - \\beta_{il})}. \\tag{8.2} \\end{equation}\\] An example PCM is shown in Figure 8.2 by plotting the probabilities of observing each of three categories as a function of \\(\\theta_p\\) (analogous to the ICC curves above). Figure 8.2: Category response functions for the PCM. 8.2 Data The vast majority of forensic decision-making occurs in casework, information about which is not often made available to researchers due to privacy concerns. Outside of casework, data on forensic science decision-making is collected through proficiency test results and in error rate studies. Proficiency tests are periodic competency exams that must be completed for forensic laboratories to maintain their accreditation. Error rate studies are independent research studies designed to measure casework error rates. As their names suggest, these two data collection scenarios serve completely different purposes. Proficiency tests are designed to assess basic competency of individuals, and mistakes are rare. Error rate studies are designed to mimic the difficulty of evidence in casework and estimate the overall error rate, aggregating over many individuals, and mistakes are more common by design. Proficiency exams consist of a large number of participants (often \\(&gt;400\\)) responding to a small set of questions (often \\(&lt;20\\)). Since every participant answers every question, we can assess participant proficiency and question difficulty using the observed scores. As proficiency exams are designed to assess basic competency, most questions are relatively easy and the vast majority of participants score 100%. Error rate studies, on the other hand, consist of a smaller number of participants (fewer than \\(200\\)) and a larger pool of questions (more than \\(500\\)). The questions are designed to be difficult, and every participant does not answer every question, which makes determining participant proficiency and question difficulty a more complicated task. Results from both proficiency tests and error rate studies can be represented as a set of individuals responding to several items, in which responses can be scored as correct or incorrect. This is not unlike an educational testing scenario where students (individuals) answer questions (items) either correctly or incorrectly. There is a rich body of statistical methods for estimating student proficiency and item difficulty from test responses. Item Response Theory (IRT) is used extensively in educational testing to study the relationship between an individual’s (unobserved) proficiency and their performance on varying tasks. IRT is an especially useful tool to estimate participant proficiencies and question difficulties when participants do not necessarily answer the same set of questions. 8.3 R Packages The case study makes use of the blackboxstudyR R package (Luby 2019), which provides functions for working with the FBI black box data, implementations of basic IRT models in Stan (Guo, Gabry, and Goodrich 2018), and plotting functions for results. The primary functions of blackboxstudyR include: score_bb_data(): Scores the FBI “Black Box” data under one of five scoring schemes. irt_data_bb(): Formats the FBI “Black Box” data into a form appropriate for fitting a Stan model. fit_irt(): Wrapper for Stan to fit standard IRT models to data (does not be the FBI data). Models currently available are: Rasch Model (Section 8.1.1.1) 2PL Model (Section 8.1.1.1) Partial Credit Model (Section 8.1.1.2) plot_difficulty_posteriors and plot_proficiency_posteriors: Plot posterior intervals for difficulty and proficiency estimates, respectively. 8.4 Drawing Conclusions An IRT analysis produces estimates of both participant proficiency and item difficulty. As mentioned previously, this property is especially useful for settings where participants respond to different subsets of items, as it allows all participants to be compared on the same scale. By comparing the estimated proficiency to more traditional measures of participant performance (e.g. false positive rate or false negative rate), we can see whether there are aspects captured by proficiency that are not captured in other measures. For instance, the false positive rate and the false negative rate contain no information about the inconclusive rate, while IRT does implicitly, as it accounts for the number of question answered by each participant. In the forensic science setting, completing an IRT analysis will often include an additional step of choosing how the data should be scored. For example, should inconclusive responses be scored as incorrect or treated as missing? An additional question we may wish to answer is, “Which scoring scheme is most appropriate for the setting at hand?” In some cases, the optimal scoring scheme may be determined using expert knowledge, or by specifying the expected answers to each item beforehand. In other cases, it may not be possible to determine the optimal scoring scheme before fitting an IRT model. In those cases, multiple scoring methods should be used to fit an IRT model, and the results from each model should be compared and contrasted. 8.5 Case Study We use the FBI “black box” data (blackboxstudyR::TestResponses) for our case study. TestResponses is a data frame in which each row corresponds to an examiner, each column represents the item, and the value in each unique combination of row and column is the examiner’s response to that item. In addition to the examiner ID (Examiner_ID) and item ID (Pair_ID), the data contains: Mating: whether the pair of prints were “Mates” (same source) or “Non-mates” (different source) Latent_Value: the examiner’s assessment of the value of the print (NV = No Value, VEO = Value for Exclusion Only, VID = Value for Individualization) Compare_Value: the examiner’s assessment of whether the pair of prints is an “Exclusion”, “Inconclusive” or “Individualization” Inconclusive_Reason: If inconclusive, the reason for the inconclusive “Close”: The correspondence of features is supportive of the conclusion that the two impressions originated from the same source, but not the extent sufficient for individualization. “Insufficient”: Potentially corresponding areas are present, but there is insufficient information present. Participants were told to select this reason if the reference print was not of value. “No Overlap”: No overlapping area between the latent and reference Exclusion_Reason: If exclusion, the reason for the exclusion “Minutiae” “Pattern” Difficulty: Reported difficulty ranging from “A_Obvious” to “E_VeryDifficult” In order to fit an IRT model, we must first score the data. Responses are scored as correct if they are true identifications (Mating == Mates and Compare_Value == Individualization) or exclusions (Mating == Non-mates and Compare_Value == Exclusion). Similarly, responses are scored as incorrect if they are false identifications (Mating == Non-mates and Compare_Value == Individualization) or false exclusions (Mating == Mates and Compare_Value == Exclusion). Inconclusive responses, which are never keyed as correct responses, complicate the scoring of the exam due to both their ambiguity and prevalence. There are a large number of inconclusive answers (4907 of 17121 responses), and examiners vary on which latent print pairs are inconclusive. The blackboxstudyR package includes five methods to score inconclusive responses: Score all inconclusive responses as incorrect (inconclusive_incorrect). This may penalize participants who were shown more vague or harder questions and therefore reported more inconclusives. Treat inconclusive responses as missing completely at random (inconclusive_mcar). This decreases the amount of data included in the analysis, and does not explicitly penalize examiners who report many inconclusives. This is the scoring method most similar to the method used in Bradford T. Ulery et al. (2011) to compute false positive and false negative rates. Score inconclusive as correct if the reason given for an inconclusive is “correct”. Since the ground truth “correct” inconclusive reason is unknown, the consensus reason from other inconclusive responses for that question is used. If no consensus reason exists, the inconclusive response was scored in one of two ways: Treat inconclusive responses as incorrect if no consensus reason exists (no_consensus_incorrect). Treat inconclusive responses as missing completely at random if no consensus reason exists (no_consensus_mcar). Score inconclusive responses as “partial credit” (partial_credit). In the remainder of the case study we will 1. demonstrate how to fit an IRT model in R, 2. illustrate how IRT analysis complements an error rate analysis by accounting for participants seeing different sets of questions, and 3. show how different scoring methods can change results from an IRT analysis. 8.5.1 Fitting the IRT model We’ll proceed with an IRT analysis of the data under the inconclusive_mcar scoring scheme, which is analogous to how the data were scored under Bradford T. Ulery et al. (2011). im_scored &lt;- score_bb_data(TestResponses, &quot;inconclusive_mcar&quot;) Scoring the black box data as above gives us the response variable (\\(y\\)). The irt_data_bb function takes the original black box data, along with the scored variable produced by score_bb_data, and produces a list object in the form needed by Stan to fit the IRT models. If you wish to fit the models on a different set of data, you can do so if the dataset has been formatted as a list object with the same attributes as the irt_data_bb function output (see package documentation for additional details). im_data &lt;- irt_data_bb(TestResponses, im_scored) We can now use fit_irt to fit the Rasch models. im_model &lt;- fit_rasch(im_data, iterations = 600, n_chains = 4) In practice, it is necessary to ensure that the MCMC sampler has converged using a variety of diagnostics. We omit these steps here for brevity, but the blackboxstudyR package will include a vignette detailing this process, or see e.g. Gelman et al. (2013). After the model has been fit, we can plot the posterior distributions of difficulties and proficiencies: p1 &lt;- plot_proficiency_posteriors(im_samples) + my_theme p2 &lt;- plot_difficulty_posteriors(im_samples) + my_theme ggarrange(p1, p2, ncol = 2) The lighter gray interval represents the 95% posterior interval and the black interval represents the 50% posterior interval. If we examine the posterior intervals for the difficulty estimates (\\(b\\)), we can see groups which have noticeably larger intervals, and thus more uncertainty regarding the estimate: 1. those on the bottom left 2. those on the upper right, and 3. those in the middle. These three groups of uncertain estimates correspond to: 1. the questions that every participant answered correctly, 2. the questions that every participant answered incorrectly, and 3. the questions that every participant reported as an “inconclusive” or “no value”. 8.5.2 IRT complements an error rate analysis The original analysis of the FBI “Black Box” Study (see Bradford T. Ulery et al. 2011) did not include analysis of participant error rates, because each participant saw a different question set. Since proficiency accounts for the difficulty of question sets, however, we can directly compare participant proficiencies to each other, and also see how error rates and proficiency are related. First, we compute the observed person scores. obs_p_score &lt;- bb_person_score(TestResponses, im_scored) In order to use the error_rate_analysis function, we need to extract the median question difficulties from MCMC results. q_diff &lt;- apply(im_samples, 3, median)[grep(&quot;b\\\\[&quot;, names(apply(im_samples, 3, median)))] ex_error_rates &lt;- error_rate_analysis(TestResponses, q_diff) Now, we can plot the proficiency estimates (with 95% posterior intervals) against the results from a traditional error rate analysis. p1 &lt;- person_mcmc_intervals(im_samples) %&gt;% right_join(., obs_p_score, by = &quot;exID&quot;) %&gt;% full_join(., ex_error_rates, by = &quot;exID&quot;) %&gt;% dplyr::select(., score, m, ll, hh, exID, avg_diff, fpr, fnr) %&gt;% ggplot(., aes(x = fpr, y = m, ymin = ll, ymax = hh)) + geom_pointrange(size = 0.3) + labs(x = &quot;False Positive Rate&quot;, y = &quot;Proficiency Estimate&quot;) + my_theme p2 &lt;- person_mcmc_intervals(im_samples) %&gt;% right_join(., obs_p_score, by = &quot;exID&quot;) %&gt;% full_join(., ex_error_rates, by = &quot;exID&quot;) %&gt;% dplyr::select(., score, m, ll, hh, exID, avg_diff, fpr, fnr) %&gt;% ggplot(., aes(x = fnr, y = m, ymin = ll, ymax = hh, color = fpr &gt; 0)) + geom_pointrange(size = 0.3) + labs(x = &quot;False Negative Rate&quot;, y = &quot;Proficiency Estimate&quot;) + scale_colour_manual(values = c(&quot;black&quot;, &quot;steelblue&quot;)) + my_theme + theme(legend.position = &quot;none&quot;) ggarrange(p1, p2, ncol = 2) Figure 8.3: Proficiency vs False Positive Rate (left) and False Negative Rate (right) Figure 8.3 shows proficiency against the false positive rate (left) and false negative rate (right). Those participants who made at least one false positive error are colored in blue on the right side plot. We see that one of the participants who made a false positive error still received a relatively large proficiency estimate due to having such a small false negative rate. If, instead of looking error rates for each participant, we examine observed scores, the estimated proficiencies correlate with the observed score (Figure 8.4. That is, participants with a higher observed score are generally given larger proficiency estimates than participants with lower scores. There are, however, cases where participants scored roughly the same on the study but are given vastly different proficiency estimates. For example, the highlighted participants in the right plot above all scored between 94% and 96%, but their estimated proficiencies range from \\(-1.25\\) to \\(2.5\\). p1 &lt;- person_mcmc_intervals(im_samples) %&gt;% right_join(obs_p_score, by = &quot;exID&quot;) %&gt;% ggplot(aes(x = score, y = m, ymin = ll, ymax = hh)) + geom_pointrange(size = 0.3) + labs(x = &quot;Observed Score&quot;, y = &quot;Proficiency Estimate&quot;) + my_theme p2 &lt;- person_mcmc_intervals(im_samples) %&gt;% right_join(obs_p_score, by = &quot;exID&quot;) %&gt;% ggplot(aes(x = score, y = m, ymin = ll, ymax = hh)) + geom_pointrange(size = 0.3) + gghighlight(score &lt; 0.96 &amp; score &gt; 0.94) + labs(x = &quot;Observed Score&quot;, y = &quot;Proficiency Estimate&quot;) + my_theme ggarrange(p1, p2, ncol = 2) Figure 8.4: Proficiency vs Observed Score If we examine those participants who scored between 94% and 96% more closely, we can see that the discrepancies in their proficiencies are largely explained by the difficulty of the specific question set they saw. This is evidenced by the positive trend in Figure 8.5. In addition to the observed score and difficulty of the question set, the number of questions the participant answers conclusively (i.e. individualization or exclusion) also plays a role in the proficiency estimate. Participants who are conclusive more often generally receive higher estimates of proficiency than participants who are conclusive less often. person_mcmc_intervals(im_samples) %&gt;% right_join(obs_p_score, by = &quot;exID&quot;) %&gt;% full_join(ex_error_rates, by = &quot;exID&quot;) %&gt;% dplyr::select(score, m, ll, hh, exID, avg_diff, pct_skipped) %&gt;% filter(score &lt; 0.96 &amp; score &gt; 0.94) %&gt;% ggplot(aes(x = avg_diff, y = m, ymin = ll, ymax = hh, col = 1 - pct_skipped)) + geom_pointrange(size = 0.3) + labs(x = &quot;Avg Q Difficulty&quot;, y = &quot;Proficiency Estimate&quot;, color = &quot;% Conclusive&quot;) + my_theme Figure 8.5: Proficiency vs Average Question Difficulty, for participants with observed score between 94 and 96 percent correct. 8.5.3 Scoring method affects proficiency estimates To illustrate the difference in results between different scoring methods, we’ll now score the data and fit models in two more ways: no_consensus_incorrect and partial_credit. nci_scored &lt;- score_bb_data(TestResponses, &quot;no_consensus_incorrect&quot;) nci_data &lt;- irt_data_bb(TestResponses, nci_scored) pc_scored &lt;- score_bb_data(TestResponses, &quot;partial_credit&quot;) pc_data &lt;- irt_data_bb(TestResponses, pc_scored) We use fit_rasch to fit the Rasch model to the no_consensus_incorrect data, and since the partial_credit data has three outcomes (correct, inconclusive, or incorrect) instead of only two (correct/incorrect), we use fit_pcm to fit a partial credit model to the data. nci_model &lt;- fit_rasch(nci_data, iterations = 1000, n_chains = 4) pc_model &lt;- fit_pcm(pc_data, iterations = 1000, n_chains = 4) We can examine the proficiency estimates and observed scores for each participant under each of the three scoring schemes, similar to Figure 8.4 above. Under the partial credit scoring scheme, a correct identification/exclusion is scored as a “2”, an inconclusive response is scored as a “1” and an incorrect identification/exclusion is scored as a “0”. The observed score is then computed by \\((\\# Correct + \\# Inconclusive) / (2 \\times \\# Responses)\\) to scale the score to be between 0 and 1. p_score_im &lt;- bb_person_score(TestResponses, im_scored) p_score_im &lt;- person_mcmc_intervals(blackboxstudyR::im_samples) %&gt;% right_join(p_score_im, by = &quot;exID&quot;) %&gt;% mutate(scoring = rep(&quot;im&quot;, nrow(p_score_im))) p_score_nci &lt;- bb_person_score(TestResponses, nci_scored) p_score_nci &lt;- person_mcmc_intervals(blackboxstudyR::nci_samples) %&gt;% right_join(p_score_nci, by = &quot;exID&quot;) %&gt;% mutate(scoring = rep(&quot;nci&quot;, nrow(p_score_nci))) p_score_pc &lt;- bb_person_score(TestResponses, pc_scored) p_score_pc &lt;- person_mcmc_intervals(blackboxstudyR::pc_samples) %&gt;% right_join(p_score_pc, by = &quot;exID&quot;) %&gt;% mutate(scoring = rep(&quot;pc&quot;, nrow(p_score_pc))) p1 &lt;- p_score_im %&gt;% bind_rows(p_score_nci) %&gt;% bind_rows(p_score_pc) %&gt;% ggplot(aes(x = score, y = m, ymin = ll, ymax = hh, col = scoring)) + geom_pointrange(size = 0.3, alpha = 0.5) + labs(x = &quot;Observed Score&quot;, y = &quot;Estimated Proficiency&quot;) + my_theme p2 &lt;- p_score_im %&gt;% bind_rows(p_score_nci) %&gt;% bind_rows(p_score_pc) %&gt;% group_by(exID) %&gt;% ggplot(aes(x = score, y = m, ymin = ll, ymax = hh, col = scoring, group = exID)) + geom_pointrange() + gghighlight(hh &lt; -0.5, use_group_by = FALSE) + geom_line(col = &quot;black&quot;, linetype = &quot;dotted&quot;) + labs(x = &quot;Observed Score&quot;, y = &quot;Estimated Proficiency&quot;) + geom_hline(yintercept = -0.5, col = &quot;darkred&quot;, linetype = &quot;dashed&quot;) + my_theme ggarrange(p1, p2, ncol = 2, common.legend = TRUE, legend = &quot;bottom&quot;) Figure 8.6: Proficiency vs Observed Score for each of three scoring schemes Treating the inconclusives as missing (“im”), leads to both the smallest range of observed scores and largest range of estimated proficiencies. Harsher scoring methods (e.g. no consensus incorrect (“nci”)) do not necessarily lead to lower estimated proficiencies. For instance, the participants who scored around 45% under the “nci” scoring method (in green) are given higher proficiency estimates than the participant who scored 70% under the “im” scoring method. The scoring method thus affects the proficiency estimates in a somewhat non-intuitive way, as larger ranges of observed scores do not necessarily correspond to larger ranges of proficiency estimates. Also note that the uncertainty intervals under the “im” scoring scheme are noticeably larger than under the other scoring schemes. This is because the inconclusive_mcar scheme treats all of the inconclusives, nearly a third of the data, as missing. This missingness is completely uninformative when estimating the difficulty and proficiency estimates. Under the other scoring schemes (no consensus incorrect and partial credit) the inconclusive responses are never treated as missing, leading to a larger number of observations per participant and therefore a smaller amount of uncertainty in the proficiency estimate. The range of proficiencies under different scoring schemes and the uncertainty intervals for the proficiency estimates both have substantial implications if we consider setting a “mastery level” for participants. As an example, let’s consider setting the mastery threshold at \\(-0.5\\). We will then say examiners have not demonstrated mastery if the upper end of their proficiency uncertainty estimate is below \\(-0.5\\), illustrated in the right plot of Figure 8.6. The number of examiners that have not demonstrated mastery varies based on the scoring method used (11 for “nci”, 8 for “pc” and 11 for “im”) due to the variation in range of proficiency estimates. Additionally, for each of the scoring schemes, there are a number of examiners that did achieve mastery with the same observed score as those that did not demonstrate mastery. This is due to a main feature of item response models discussed earlier: participants that answered more difficult questions are given higher proficiency estimates than participants that answered the same number of easier questions. We’ve also drawn dotted lines between proficiency estimates that correspond to the same person. Note that many of the participants who do not achieve mastery under one scoring scheme do achieve mastery under the other scoring schemes, since not all of the points are connected by dotted lines. There are also a few participants who do not achieve mastery under any of the scoring schemes. This raises the question of how much the proficiency estimates change for each participant under the different scoring schemes. The plot on the left in Figure 8.7 shows both a change in examiner proficiencies across scoring schemes (the lines connecting the proficiencies are not horizontal) as well as a change in the ordering of examiner proficiencies (the lines cross one another). That is, different scoring schemes affect examiner proficiencies in different ways. The plot on the right illustrates participants that see substantial differences in their proficiency estimates under different scoring schemes. Examiners 105 and 3 benefit from the leniency in scoring when inconclusives are treated as missing (“im”). When inconclusives are scored as incorrect (“nci”) or partial credit (“pc”), they see a substantial decrease in their proficiency due to reporting a high number of inconclusives and differing from other examiners in their reasoning for reporting inconclusives. Examiners 142, 60 and 110, on the other hand, are hurt by the leniency in scoring when inconclusives are treated as missing (“im”). Their proficiency estimates increase when inconclusives are scored as correct when they match the consensus reason (“nci”) or are worth partial credit (“pc”). p1 &lt;- p_score_im %&gt;% bind_rows(p_score_nci) %&gt;% bind_rows(p_score_pc) %&gt;% arrange(parameter) %&gt;% mutate(id = rep(1:169, each = 3)) %&gt;% dplyr::select(id, m, scoring) %&gt;% spread(scoring, m) %&gt;% mutate(max.diff = apply(cbind(abs(im - nci), abs(nci - pc), abs(im - pc)), 1, max)) %&gt;% gather(&quot;model&quot;, &quot;median&quot;, -c(id, max.diff)) %&gt;% ggplot(aes(x = model, y = median, group = id, col = id)) + geom_point() + geom_line() + labs(x = &quot;Scoring Method&quot;, y = &quot;Estimated Proficiency&quot;) + my_theme p2 &lt;- p_score_im %&gt;% bind_rows(p_score_nci) %&gt;% bind_rows(p_score_pc) %&gt;% arrange(parameter) %&gt;% mutate(id = rep(1:169, each = 3)) %&gt;% dplyr::select(id, m, scoring) %&gt;% spread(scoring, m) %&gt;% mutate(max.diff = apply(cbind(abs(im - nci), abs(nci - pc), abs(im - pc)), 1, max)) %&gt;% gather(&quot;model&quot;, &quot;median&quot;, -c(id, max.diff)) %&gt;% ggplot(aes(x = model, y = median, group = id, col = id)) + geom_point() + geom_line() + labs(x = &quot;Scoring Method&quot;, y = &quot;Estimated Proficiency&quot;) + gghighlight(max.diff &gt; 1.95, label_key = id, use_group_by = FALSE) + my_theme ggarrange(p1, p2, ncol = 2, common.legend = TRUE, legend = &quot;bottom&quot;) Figure 8.7: Change in proficiency for each examiner under the three scoring schemes. The right side plot has highlighted five examiners whose proficiency estimates change the most across schemes. 8.5.4 Discussion We have provided an overview of human decision-making in forensic analyses, through the lens of latent print comparisons and the FBI “black box” study (Bradford T. Ulery et al. 2011). A brief overview of Item Response Theory (IRT), a class of models used extensively in educational testing, was introduced in Section 8.1.1. A case study is provided of an IRT analysis on the FBI ``black box’’ study in 8.5. Results from an IRT analysis are largely consistent with conclusions from an error rate analysis. However, IRT provides substantially more information than a more traditional analysis, specifically through accounting for the difficulty of questions seen. Additionally, IRT implicitly accounts for the inconclusive rate of different participants and provides estimates of uncertainty for both participant proficiency and item difficulty. If IRT were to be adopted on a large scale, participants would be able to be directly compared even if they took different exams (for instance, proficiency exams in different years). Three scoring schemes were presented in the case study, each of which leads to substantially different proficiency estimates across participants. Although IRT is a powerful tool for better understanding examiner performance on forensic identification tasks, we must be careful when choosing a scoring scheme. This is especially important for analyzing ambiguous responses, such as the inconclusive responses in the “black box” study. References "],
["acknowledgements-2.html", "Acknowledgements", " Acknowledgements This book was made possible by an award from rOpenSci, a fiscally sponsored project of NumFOCUS. The PI on the award is Heike Hofmann, my Ph.D. advisor, turned colleague and friend. Thank you, Heike! Big thanks go to Karthik Ram and the rOpenSci awards committee, Di Cook, Mine Cetinkaya-Rundel, Matt Jones, and Ken Benoit, for awarding us this fellowship. Thank you also to the many contributors of this book’s chapters, who have written something or have given me their time, guidance, and patience: Soyoung Park Xiao Hui Tai Amanda Luby Karen Pan Ganesh Krishnan Eric Hare Sarah Riman Finally, thank you to my wonderful PI, Alicia Carriquiry, who supported this project as a part of my postdoctoral appointment at CSAFE. Thank you, Sam C. Tyner, Ph.D. "],
["glossary.html", "A Glossary A.1 Introduction A.2 Validation of DNA Interpretation Systems A.3 Firearms: bullets A.4 Firearms: casings A.5 Latent Fingerprints A.6 Shoe Outsole Impression Evidence A.7 Trace Glass Evidence A.8 Decision-making in Forensic Identification Tasks", " A Glossary Terms are given in the order in which they appear in the body of the text. A.1 Introduction A.1 : Forensics - The art or study of argumentation and formal debate. Source: dictionary.com A.2 : Forensic Science - Any science used for the purposes of the law. Source: American Academy of Forensic Sciences) A.3 : R - R is a free software environment for statistical computing and graphics. Source: The R Project for Statistical Computing A.4 : Integrated Development Environment (IDE) - An IDE increases programmer productivity by combining common activities of writing software into a single application: editing source code, building executables, and debugging. Source: Codecademy A.5 : Open Source Software - Open source software is software with source code that anyone can inspect, modify, and enhance. Source: opensource.com A.6 : Proprietary - Of a brand name, product, service, formula, etc.: protected by a patent, copyright, or trademark. Source: dictionary.com A.7 : Source Code - The part of software that most computer users don’t ever see; it’s the code computer programmers can manipulate to change how a piece of software—a “program” or “application”—works. Source: opensource.com A.8 : Evidence - Data presented to a court or jury in proof of the facts in issue and which may include the testimony of witnesses, records, documents, or objects. Source: dictionary.com A.9 : Score - The quanitification of similarity between two items via a univariate function meant to inform whether the two items have a common source. Source: Hepler et al. (2012) A.10 : Likelihood Ratio - Statistical measurement evaluating the probability of evidence under one hypotheis, divided by the probability of evidence under an alternative, mutually exclusive hypothesis. In forensic science, the value of the likelihood ratio expresses the weight and meaning of scientific evidence and is used as a measure of probative value. Source: CSAFE A.2 Validation of DNA Interpretation Systems A.11 : DNA profiling - A method of identifying an individual through comparison of patterns arising from differences in DNA sequences, represented as a string of values compiles from the results of DNA testings at one or more genetic markers. Source: Butler (2009) A.12 : Polymerase Chain Reaction (PCR) - An in vitro process that yields millions of copies of desired DNA through repeated cycling of a reaction involving the DNA polymerase enzyme. Source: Butler (2009) A.13 : Short Tandem Repeats (STRs) - Multiple copies of an identical DNA sequence arranged in direct succession where the repeat sequence unit is 2 to 6 base pairs in length. Source: Butler (2009) A.14 : Single Nucleotide Polymorphism (SNP) - Any polymorphic variation at a single nucleotide. Source: Butler (2009) A.15 : Mitochondrial DNA (mtDNA) - A small, circular DNA molecule located in the mitocondria that contains approximately 16,500 nucleotides. The abundance of hundreds of copies of mtDNA in each cell make it useful with samples originating from limited or damaged biological material. Source: Butler (2009) A.16 : Chromosome - The structure by which hereditary information is physically transmitted from one generation to the next. Source: Butler (2009) A.17 : Locus - A unique physical location of a gene (or specific sequence of DNA) on a chromosome. The plural of locus is loci. Source: Butler (2009) A.18 : Nucleotide - A unit of nucleic acid composed of phospate, ribose or deoxyribose, and a purine or pyrimidine base Source: Butler (2009) A.19 : Nucleic Acid - A general class of molecules that are polymers of nucleotides. DNA and RNA are the major types. Source: Butler (2009) A.20 : Database - A comprehensive collection of related data organized forconvenient access, generally in a computer. Source: dictionary.com A.21 : Capillary Electrophoresis (CE) - An electrophoretic technique for separating DNA molecules by their size based on migration through a narrow glass capillary tube filled with a liquid polymer. Source: Butler (2009) A.22 : Allele - An alternative form of a gene or a section of DNA at a particluar gentic location (locus). Typically, multiple alleles are possible for each STR marker. Source: Butler (2009) A.23 : Stutter product - Either (n-4) or (n+4). If (n-4), a minor peak appearing one repeat unit smapper than a primary STR allele resulting from strang slippage during the amplification process. If (n+4), a minor peak appearing one repeat unit larger than a primary STR allele resulting from strand slippage during the amplification process. Source: Butler (2009) A.24 : Graphical User Interface (GUI) - A form of user interface that allows users to interact with electronic devices through graphical icons and visual indicators such as secondary notation, instead of text-based user interfaces. Source: Wikipedia A.3 Firearms: bullets A.25 : Bullet - A kinetic projectile and the component of firearm ammunition that is expelled from the gun barrel during shooting Source: Wikipedia A.26 : Gun Barrel - The straight shooting tube through which a contained rapid expansion of high-pressure gas(es) is introduced behind a projectile in order to propel it out of the front end at a high velocity. Source: Wikipedia A.27 : Bullet Striations - Where a bullet is pushed through the barrel of a rifled firearm, spiral grooves cut into the inside of the barrel (rifling) cause the bullet to spin. This allows increased accuracy of the shot. On the way out of the barrel, the bullet also acquires incidental markings, or striations, which may help in identifying the gun which fired a bullet. Source: CSAFE A.28 : Breech face - In a firearm, the front part of the breechblock that makes contact with the cartridge. The breechblock is what holds a round in the chamber and absorbs the recoil of the cartridge when the round is fired, preventing the cartridge case from moving. Source: CSAFE A.29 : Land engraved areas (LEAs) - The impressions on a bullet from the lands of the barrel. See image at right. Source: Scientific American A.30 : Groove engraved areas (GEAs) - The impressions on a bullet from the grooves of the barrel. See image at right. Source: Scientific American A.31 : Metrology - The science of weights and measures. Source: dictionary.com A.32 : Rifling of a gun barrel - Helical grooves in the bore of a firearm barrel to impart rotary motion to a projectile. Source: National Forensic Science Technology Center A.33 : Lands and Grooves - The lands are the raised areas between two grooves in the barrel of a firearm. See image at right. Source: FirearmsID.com A.34 : Cross-Correlation - A measure of similarity of two series as a function of the displacement of one relative to the other. Source: Wikipedia A.35 : Consecutively Matching Striae - Striated markings that line up exactly with one another without a break or dissimilarity in between them (Biasotti 1959). This and other forensic science papers using CMS typically count a single peak as a striae, while we count peaks and valleys, so our definition typically yields CMS values about twice those commonly found in the literature. Source: E. R. Hare (2017) A.36 : Non-matching Striation Count - Striated markings that do not line up exactly with another, without matching striation between them. Source: E. R. Hare (2017) A.37 : Cross-Correlation Function Value - The maximum value of the Cross-Correlation function evaluated at the optimal alignment. Source: E. R. Hare (2017) A.38 : Sequence Average Matching (SAM) - Using the CCF value between 2 lands, compute the average CCF value across all sequences of comparisons. The sequence with the highest average value is the “matching” sequence. If this value is greater than 0.5, then the bullets are declared to “match”. Source: Sensofar Metrology A.4 Firearms: casings A.39 : Cartridge case - See image at right. A pre-assembled firearm ammunition packaging a projectile (1), a propellant substance (3) and an ignition device within a metallic, paper or plastic case (2) that fits within the barrel chamber of a breechloading gun, for the practical purpose of convenient transportation and handling during shooting. Source: Wikipedia A.40 : Firing Pin - A part of the firing mechanism used in a firearm or explosive device. Firing pins used in firearms usually have a small, rounded portion designed to strike the primer of a cartridge, detonating the priming compound, which then ignites the propellant (inside) or fires the detonator and booster. Source: Wikipedia A.41 : Primer - A metal cup containing a primary explosive inserted into a recess in the center of the base of the cartridge. Source: Wikipedia A.42 : Individualize - Theoretically, a determination that two samples derive from the same source; practically, a determination that two samples derive from sources that cannot be distinguished at the level of analysis possible in the comparison process. Source: OSAC Lexicon A.43 : Pistol slide - A part on semi-automatic pistols that moves during the operating cycle and generally houses the firing pin/striker and the extractor, and serves as the bolt. It is spring-loaded so that once it has moved to its rearmost position in the firing cycle, spring tension brings it back to the starting position chambering a fresh cartridge during the motion provided that the magazine is not empty. Source: Wikipedia A.5 Latent Fingerprints A.44 : Latent fingerprint - A fingerprint left on a surface by deposits of oils and/or perspiration from the finger. It is not usually visible to the naked eye but may be detected with special techniques such as dusting with fine powder and then lifting the pattern of powder with transparent tape. Source: USLegal.com A.45 : Minutiae - The points where the ridge lines in a fingerprint end or fork. They can be of many types, such as ridge ending, ridge bifurcation, or crossovers. Source: Bayometric A.46 : Patent Prints - Patent prints are those fingerprints that are easily spotted without the use of magnesium powders, ultra violet lights or chemicals that might assist in the visualisation of such a print. Patent prints are often found perhaps in blood, ink, oil or on surfaces such as glass, wooden doorframes or paper. Source: Explore Forensics A.47 : Exemplar Prints - Exemplar prints, or known prints, is the name given to fingerprints deliberately collected from a subject, whether for purposes of enrollment in a system or when under arrest for a suspected criminal offense. During criminal arrests, a set of exemplar prints will normally include one print taken from each finger that has been rolled from one edge of the nail to the other, plain (or slap) impressions of each of the four fingers of each hand, and plain impressions of each thumb. Exemplar prints can be collected using live scan or by using ink on paper cards. Source: Wikipedia A.6 Shoe Outsole Impression Evidence A.48 : Shoe Outsole - The part of the shoe that is in direct contact with the ground Source: Wikipedia A.49 : Adhesive Lifter - Any material coated with a tacky substance for the purpose of lifting footwear or fingerprint impressions. Source: SWGTREAD A.50 : Electrostatic Lifting - The process of using an electrostatic charge to transfer dry origin impressions from the substrate to a film. Source: SWGTREAD A.51 : Cast - The result of filling a three-dimensional impression with an appropriate material Source: SWGTREAD A.52 : Class Characteristics - Distinctive features shared by many items of the same type. This could be a result from a manufacturing process, such as physical size, design and mold characteristics. Class characteristics can determine to which group an item belongs but does not discriminate one item in a group from another item in the same group. Source: CSAFE A.53 : Summary statistic - A value which summarizes a set of observations in order to communicate the largest amount of information as simply as possible. Source: Wikipedia A.54 : Subclass Characteristics - Features that may be produced during manufacturing that are consistent among items created by the same material in the same approximate state of wear. For example, all guns manufactured right before sharpening or cleaning the machinery. Sub-class characteristics are more restrictive than class characteristics and are not determined prior to manufacturing. Source: CSAFE A.55 : Randomly Acquired Characteristics (RACs) - Identifying characteristics that do not result from the manufacturing process, but are accidental, unpredictable characteristics that result from wear. These could include objects that have become attached to the outsole of a shoe—such as rocks, thumb tacks, or tape—or marks on a bullet or casing caused by lack of cleaning of a gun. Source: CSAFE A.56 : Maximal Clique - Given a group of vertices (points) some of which have edges (lines) in between them, the maximal clique is the largest subset of vertices in which each point is directly connected to every other vertex in the subset. Source: Stanford Computer Science A.7 Trace Glass Evidence A.57 : Questioned Source - Evidence of unknown origin. These samples could be found at a crime scene, transferred to an offender during commission of a crime, or recovered from more than one crime scene. Source: forensicsciencesimplified.org A.58 : Known Source - Also called a reference sample, this is material from a verifiable/documented source which, when compared with evidence of a questioned source, shows an association or linkage between an offender, crime scene, and/or victim. Source: forensicsciencesimplified.org A.59 : Specific source question - The specific source identification question considers whether the trace originates from a fixed specific source. Source: Ommen, Saunders, and Neumann (2015) A.60 : Refractive Index - A dimensionless number that describes how fast light propagates through a material. Source: Wikipedia A.61 : Float Glass - Extremely smooth, nearly distortion-free plate glass manufactured by pouring molten glass onto asurface of molten tin. Source: dictionary.com A.62 : ASTM International - An international standards organization that develops and publishes voluntary consensus technical standards for a wide range of materials, products, systems, and services. Source: Wikipedia A.63 : Random Forest - An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Source: Wikipedia A.64 : Fluorescence - The emission of radiation, especially of visible light, by a substance during exposure to externalradiation, as light or x-rays. Source: dictionary.com A.65 : Inductively Coupled Mass Spectrometry with a Laser Add-on (LA-ICP-MS) - A powerful analytical technology that enables highly sensitive elemental and isotopic analysis to be performed directly on solid samples. LA-ICP-MS begins with a laser beam focused on the sample surface to generate fine particles – a process known as Laser Ablation. The ablated particles are then transported to the secondary excitation source of the ICP-MS instrument for digestion and ionization of the sampled mass. The excited ions in the plasma torch are subsequently introduced to a mass spectrometer detector for both elemental and isotopic analysis. Source: Applied Spectra A.66 : Machine Learning - The scientific study of algorithms and statistical models that computer systems use to effectively perform a specific task without using explicit instructions, relying on patterns and inference instead Source: Wikipedia A.67 : Classifier - An algorithm which takes in data observations and outputs a prediction about which class or group in the population each observation belongs to. Source: Wikipedia A.68 : Features - An individual measurable property or characteristic of a phenomenon being observed Source: Wikipedia A.69 : Receiver Operating Characteristic Curve - A commonly used summary for assessing the tradeoff between sensitivity (true positive rate) and specificity (true negative rate) of a classifier. Source: Friedman, Hastie, and Tibshirani (2001) A.70 : K-fold Cross-Validation - A method to assess performance of a prediction model. This method splits the data up into K parts, fits the model to K-1 parts of the data, then predicts on the leftover part of data K times. Source: Friedman, Hastie, and Tibshirani (2001) A.71 : Sensitivity - A measure of performance for a 2 class classifier. It is equal to the true positive rate, the probability of predicting a positive response when the truth is positive. Source: Friedman, Hastie, and Tibshirani (2001) A.72 : Specificity - A measure of performance for a 2 class classifier. It is equal to the true negative rate, the probability of predicting a negative response when the truth is negative. Source: Friedman, Hastie, and Tibshirani (2001) A.8 Decision-making in Forensic Identification Tasks A.73 : Black box study - Black-Box Study—A black box study assesses the accuracy of examiners’ conclusions without considering how the conclusions were reached. The examiner is treated as a “black-box” and the researcher measures how the output of the “black-box” (examiner’s conclusion) varies depending on the input (the test specimens presented for analysis). To test examiner accuracy, the “ground truth” regarding the type or source of the test specimens must be known with certainty. Source: OSAC Human Factors Committee References "],
["references.html", "References", " References "]
]
